{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f3dd12f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "# Train your first CARE network (supervised)\n",
    "\n",
    "In this first example we will train a CARE network for a 2D denoising and upsampling task, where corresponding pairs of low and high signal-to-noise ratio (SNR) images of cells are available. Here the high SNR images are acquistions of Human U2OS cells taken from the [Broad Bioimage Benchmark Collection](https://data.broadinstitute.org/bbbc/BBBC006/) and the low SNR images were created by synthetically adding *strong read-out and shot-noise* and applying *pixel binning* of 2x2, thus mimicking acquisitions at a very low light level.\n",
    "\n",
    "![](nb_material/denoising_binning_overview.png)\n",
    "\n",
    "\n",
    "For CARE image pairs should be registered, which in practice is best achieved by acquiring both stacks _interleaved_, i.e. as different channels that correspond to the different exposure/laser settings.\n",
    "\n",
    "Since the image pairs were synthetically created in this example, they are already aligned perfectly.\n",
    "\n",
    "To train a denoising network we will use the [CSB Deep Repo](https://github.com/CSBDeep/CSBDeep). This notebook has a very similar structure to the examples you can find there.\n",
    "More documentation is available at http://csbdeep.bioimagecomputing.com/doc/.\n",
    "\n",
    "This part will no tasks to fill in in this exercise, but go through each cell and try to understand what's going on - it will help you in the next part! We also put some questions along the way. For some of them you might need to dig a bit deeper.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "Set your python kernel to <code>03_image_restoration_part1</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7396e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from csbdeep.data import (\n",
    "    RawData,\n",
    "    create_patches,\n",
    "    no_background_patches,\n",
    "    norm_percentiles,\n",
    "    sample_percentiles,\n",
    ")\n",
    "from csbdeep.io import load_training_data, save_tiff_imagej_compatible\n",
    "from csbdeep.models import CARE, Config\n",
    "from csbdeep.utils import (\n",
    "    Path,\n",
    "    axes_dict,\n",
    "    download_and_extract_zip_file,\n",
    "    normalize,\n",
    "    plot_history,\n",
    "    plot_some,\n",
    ")\n",
    "from csbdeep.utils.tf import limit_gpu_memory\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from tifffile import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275e8f32",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Part 1: Training Data Generation\n",
    "Network training usually happens on batches of smaller sized images than the ones recorded on a microscopy. In this first part of the exercise, we will load all of the image data and chop it into smaller pieces, a.k.a. patches.\n",
    "\n",
    "### Download example data\n",
    "\n",
    "First we download some example data, consisting of low-SNR and high-SNR 3D images of Tribolium.\n",
    "Note that `GT` stands for ground truth and represents high signal-to-noise ratio (SNR) stacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa838fb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "download_and_extract_zip_file(\n",
    "    url=\"https://zenodo.org/record/6973411/files/care_denoising_upsampling.zip?download=1\",\n",
    "    targetdir=\"data/U2OS/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd51b24",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "As we can see, the data set is already split into a **train** and **test** set, each containing (synthetically generated) low SNR (\"low\") and corresponding high SNR (\"GT\") images.\n",
    "\n",
    "Let's look at an example pair of training images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc18e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = imread(\"data/U2OS/train/GT/img_0010.tif\")\n",
    "x = imread(\"data/U2OS/train/low/img_0010.tif\")\n",
    "print(\"GT image size =\", x.shape)\n",
    "print(\"low image size =\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f8b9c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(x, cmap=\"magma\")\n",
    "plt.colorbar()\n",
    "plt.title(\"low\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(y, cmap=\"magma\")\n",
    "plt.colorbar()\n",
    "plt.title(\"high\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db1f30",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Generate training data for CARE\n",
    "\n",
    "We first need to create a `RawData` object, which defines how to get the pairs of low/high SNR stacks and the semantics of each axis (e.g. which one is considered a color channel, etc.). In general the names for the axes are:\n",
    "\n",
    "X: columns, Y: rows, Z: planes, C: channels, T: frames/time, (S: samples/images)\n",
    "\n",
    "Here we have two folders \"low\" and \"GT\", where corresponding low and high-SNR stacks are TIFF images with identical filenames.\n",
    "\n",
    "For this case, we can simply use `RawData.from_folder` and set `axes = 'YX'` to indicate the semantic order of the image axes, i.e. we have two-dimensional images in standard xy layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5052615",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "raw_data = RawData.from_folder(\n",
    "    basepath=\"data/U2OS/train\",\n",
    "    source_dirs=[\"low\"],\n",
    "    target_dir=\"GT\",\n",
    "    axes=\"YX\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc41015",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "From corresponding images, we now generate some 2D patches to use for training.\n",
    "\n",
    "As a general rule, use a *patch size* that is a power of two along all axes, or at least divisible by 8.  Typically, you should use more patches the more trainings images you have.\n",
    "\n",
    "An important aspect is *data normalization*, i.e. the rescaling of corresponding patches to a dynamic range of ~ (0,1). By default, this is automatically provided via percentile normalization, which can be adapted if needed.\n",
    "\n",
    "By default, patches are sampled from *non-background regions* (i.e. that are above a relative threshold). We will disable this for the current example as most image regions already contain foreground pixels and thus set the threshold to 0. See the documentation of `create_patches` for details.\n",
    "\n",
    "Note that returned values `(X, Y, XY_axes)` by `create_patches` are not to be confused with the image axes X and Y. By convention, the variable name X (or x) refers to an input variable for a machine learning model, whereas Y (or y) indicates an output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695bfb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, XY_axes = create_patches(\n",
    "    raw_data=raw_data,\n",
    "    patch_size=(128, 128),\n",
    "    patch_filter=no_background_patches(0),\n",
    "    n_patches_per_image=2,\n",
    "    save_file=\"data/U2OS/my_training_data.npz\",\n",
    ")\n",
    "\n",
    "assert X.shape == Y.shape\n",
    "print(\"shape of X,Y =\", X.shape)\n",
    "print(\"axes  of X,Y =\", XY_axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0986b4",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Show\n",
    "\n",
    "This shows some of the generated patch pairs (odd rows: *source*, even rows: *target*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598742b6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    sl = slice(8 * i, 8 * (i + 1)), 0\n",
    "    plot_some(\n",
    "        X[sl], Y[sl], title_list=[np.arange(sl[0].start, sl[0].stop)]\n",
    "    )  # convenience function provided by CSB Deep\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9030a3c9",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Questions:</h3>\n",
    "    <ol>\n",
    "        <li>Where is the training data located?</li>\n",
    "        <li>How is the data organized to identify the pairs of HR and LR images?</li>\n",
    "    </ol>\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Part 2: Training the network\n",
    "\n",
    "\n",
    "### Load Training data\n",
    "\n",
    "Load the patches generated in part 1, use 10% as validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40de3c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, Y), (X_val, Y_val), axes = load_training_data(\n",
    "    \"data/U2OS/my_training_data.npz\", validation_split=0.1, verbose=True\n",
    ")\n",
    "\n",
    "c = axes_dict(axes)[\"C\"]\n",
    "n_channel_in, n_channel_out = X.shape[c], Y.shape[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d3cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plot_some(X_val[:5], Y_val[:5])\n",
    "plt.suptitle(\"5 example validation patches (top row: source, bottom row: target)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fbdec0",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Configure the CARE model\n",
    "Before we construct the actual CARE model, we have to define its configuration via a `Config` object, which includes\n",
    "* parameters of the underlying neural network,\n",
    "* the learning rate,\n",
    "* the number of parameter updates per epoch,\n",
    "* the loss function, and\n",
    "* whether the model is probabilistic or not.\n",
    "\n",
    "![](nb_material/carenet.png)\n",
    "\n",
    "The defaults should be sensible in many cases, so a change should only be necessary if the training process fails.\n",
    "\n",
    "<span style=\"color:red;font-weight:bold;\">Important</span>: Note that for this notebook we use a very small number of update steps for immediate feedback, whereas the number of epochs and steps per epoch should be increased considerably (e.g. `train_steps_per_epoch=400`, `train_epochs=100`) to obtain a well-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d756073",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    axes,\n",
    "    n_channel_in,\n",
    "    n_channel_out,\n",
    "    train_batch_size=8,\n",
    "    train_steps_per_epoch=40,\n",
    "    train_epochs=20,\n",
    ")\n",
    "vars(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33a8ad5",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "We now create a CARE model with the chosen configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a192b10c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "model = CARE(config, \"my_CARE_model\", basedir=\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5879e9c9",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "We can get a summary of all the layers in the model and the number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db80493",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "model.keras_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a307eb78",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Training\n",
    "\n",
    "Training the model will likely take some time. We recommend to monitor the progress with [TensorBoard](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard), which allows you to inspect the losses during training.\n",
    "Furthermore, you can look at the predictions for some of the validation images, which can be helpful to recognize problems early on.\n",
    "\n",
    "We can start tensorboard within the notebook.\n",
    "<div class=\"alert alert-danger\">\n",
    "Note that if you're using ssh instead of nomachine you will need to forward the port that tensorboard picks (6006 by default, but can be different), i.e. running something that looks like this:\n",
    "<code>ssh -NL 6006:localhost:6006 &lt;user&gt;@&lt;hostname&gt;</code>\n",
    "</div>\n",
    "Alternatively, you can launch the notebook in an independent tab by changing the `%` to `!`\n",
    "<div class=\"alert alert-danger\">\n",
    "If you're using ssh add <code>--host &lt;hostname&gt;</code> to the command. (This has the advantage that you don't need to first check what port tensorboard picks):\n",
    "<code>! tensorboard --logdir models --host &lt;hostname&gt;</code> where <code>&lt;hostname&gt;</code> is the thing that ends in amazonaws.com.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0368e847",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef9184c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "history = model.train(X, Y, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c5da32",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Plot final training history (available in TensorBoard during training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df6e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(history.history.keys())))\n",
    "plt.figure(figsize=(16, 5))\n",
    "plot_history(history, [\"loss\", \"val_loss\"], [\"mse\", \"val_mse\", \"mae\", \"val_mae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da6852f",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Evaluation\n",
    "Example results for validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801b0e3f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "_P = model.keras_model.predict(X_val[:5])\n",
    "if config.probabilistic:\n",
    "    _P = _P[..., : (_P.shape[-1] // 2)]\n",
    "plot_some(X_val[:5], Y_val[:5], _P, pmax=99.5)\n",
    "plt.suptitle(\n",
    "    \"5 example validation patches\\n\"\n",
    "    \"top row: input (source),  \"\n",
    "    \"middle row: target (ground truth),  \"\n",
    "    \"bottom row: predicted from source\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e9f3cf",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Questions:</h3>\n",
    "    <ol>\n",
    "        <li>Where are trained models stored? What models are being stored, how do they differ?</li>\n",
    "        <li>How does the name of the saved models get specified?</li>\n",
    "        <li>How can you influence the number of training steps per epoch? What did you use?</li>\n",
    "    </ol>\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Part 3: Prediction\n",
    "\n",
    "Plot the test stack pair and define its image axes, which will be needed later for CARE prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87808d52",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "y_test = imread(\"data/U2OS/test/GT/img_0010.tif\")\n",
    "x_test = imread(\"data/U2OS/test/low/img_0010.tif\")\n",
    "\n",
    "axes = \"YX\"\n",
    "print(\"image size =\", x_test.shape)\n",
    "print(\"image axes =\", axes)\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "plot_some(np.stack([x_test, y_test]), title_list=[[\"low\", \"high\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a62e577",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Load CARE model\n",
    "\n",
    "Load trained model (located in base directory `models` with name `my_CARE_model`) from disk.\n",
    "The configuration was saved during training and is automatically loaded when `CARE` is initialized with `config=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2cdd19",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "model = CARE(config=None, name=\"my_CARE_model\", basedir=\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80952479",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Apply CARE network to raw image\n",
    "Predict the restored image (image will be successively split into smaller tiles if there are memory issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a828b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "restored = model.predict(x_test, axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a94f8",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "### Save restored image\n",
    "\n",
    "Save the restored image stack as a ImageJ-compatible TIFF image, i.e. the image can be opened in ImageJ/Fiji with correct axes semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a39009",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"results\").mkdir(exist_ok=True)\n",
    "save_tiff_imagej_compatible(\"results/%s_img_0010.tif\" % model.name, restored, axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ab3027",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Visualize results\n",
    "Plot the test stack pair and the predicted restored stack (middle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc551a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plot_some(\n",
    "    np.stack([x_test, restored, y_test]),\n",
    "    title_list=[[\"low\", \"CARE\", \"GT\"]],\n",
    "    pmin=2,\n",
    "    pmax=99.8,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for _x, _name in zip((x_test, restored, y_test), (\"low\", \"CARE\", \"GT\")):\n",
    "    plt.plot(normalize(_x, 1, 99.7)[180], label=_name, lw=2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf9934",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<hr style=\"height:2px;\">\n",
    "<div class=\"alert alert-block alert-success\"><h1>\n",
    "    Congratulations!</h1>\n",
    "    <p>\n",
    "    <b>You have reached the first checkpoint of this exercise! Please mark your progress on element!</b>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">\n",
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Bonus Questions:</h3>\n",
    "    <p>\n",
    "    Feel free to skip these\n",
    "    </p>\n",
    "    <ol>\n",
    "        <li>How would you load an existing CARE network and continue to train it? (This is _not_ done in this notebook)</li>\n",
    "        <li>How would you go about saving the new model separately, under a new name?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
