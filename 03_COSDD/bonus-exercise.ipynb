{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec02d127",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# ruff: noqa: F811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus exercise. Generating new images with COSDD\n",
    "\n",
    "COSDD is a deep generative model that captures the structures and characteristics of our data. In this notebook, we'll see how accurately it can represent our training data, in both the signal and the noise. We'll do this by using the model to generate entirely new images. These will be images that look like the ones in our training data but don't actually exist. This is the same as how models like DALL-E can generate entirely new images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "Set your python kernel to <code>05_image_restoration</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import interactive_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from COSDD import utils\n",
    "from COSDD.models.get_models import get_models\n",
    "from COSDD.models.hub import Hub\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "paths = \"../data\"\n",
    "patterns = \"mito-confocal-lowsnr.tif\"\n",
    "axes = \"SYX\"\n",
    "n_dimensions = 2\n",
    "low_snr, original_sizes = utils.load_data(\n",
    "    paths=paths, patterns=patterns, axes=axes, n_dimensions=n_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 1.1.\n",
    "\n",
    "Load the model trained in the first notebook by entering your `model_name`, or alternatively, uncomment line 4 to load the pretrained model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = ...  # Enter the model name here\n",
    "checkpoint_path = os.path.join(\"checkpoints\", model_name)\n",
    "\n",
    "# checkpoint_path = \"checkpoints/mito-confocal-pretrained\"\n",
    "\n",
    "with open(os.path.join(checkpoint_path, \"training-config.yaml\")) as f:\n",
    "    train_cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "lvae, ar_decoder, s_decoder, direct_denoiser = get_models(train_cfg, low_snr.shape[1])\n",
    "\n",
    "hub = Hub.load_from_checkpoint(\n",
    "    os.path.join(checkpoint_path, \"final_model.ckpt\"),\n",
    "    vae=lvae,\n",
    "    ar_decoder=ar_decoder,\n",
    "    s_decoder=s_decoder,\n",
    "    direct_denoiser=direct_denoiser,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0803d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mito-confocal\"\n",
    "checkpoint_path = os.path.join(\"checkpoints\", model_name)\n",
    "\n",
    "# checkpoint_path = \"checkpoints/mito-confocal-pretrained\"\n",
    "\n",
    "with open(os.path.join(checkpoint_path, \"training-config.yaml\")) as f:\n",
    "    train_cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "lvae, ar_decoder, s_decoder, direct_denoiser = get_models(train_cfg, low_snr.shape[1])\n",
    "\n",
    "hub = Hub.load_from_checkpoint(\n",
    "    os.path.join(checkpoint_path, \"final_model.ckpt\"),\n",
    "    vae=lvae,\n",
    "    ar_decoder=ar_decoder,\n",
    "    s_decoder=s_decoder,\n",
    "    direct_denoiser=direct_denoiser,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating new noise for a real noisy image\n",
    "\n",
    "First, we'll pass a noisy image to the VAE and generate a random sample from the AR decoder. This will give us another noisy image with the same underlying clean signal but a different random sample of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_image = low_snr[:1, :, :512, :512].cuda()\n",
    "reconstructions = hub.reconstruct(inp_image)\n",
    "denoised = reconstructions[\"s_hat\"].cpu()\n",
    "noisy = reconstructions[\"x_hat\"].cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 2.1.\n",
    "\n",
    "Now we will look at the original noisy image and the generated noisy image. Adjust `top`, `bottom`, `left` and `right` to view different crops of the reconstructed image.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = np.percentile(inp_image.cpu(), 1)\n",
    "vmax = np.percentile(inp_image.cpu(), 99)\n",
    "\n",
    "num_channels = inp_image.shape[1]\n",
    "max_height = inp_image.shape[2]\n",
    "max_width = inp_image.shape[3]\n",
    "\n",
    "vertical_widget = widgets.IntRangeSlider(\n",
    "    description=\"Vertical crop\",\n",
    "    min=0,\n",
    "    max=max_height,\n",
    "    step=1,\n",
    "    value=[0, max_height],\n",
    "    orientation=\"vertical\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(margin='0 0 0 30px')\n",
    ")\n",
    "horizontal_widget = widgets.IntRangeSlider(\n",
    "    description=\"Horizontal crop\",\n",
    "    min=0,\n",
    "    max=max_width,\n",
    "    step=1,\n",
    "    value=[0, max_width],\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "### Explore slices of the data here\n",
    "def plot_crop(horizontal, vertical):\n",
    "    left, right = horizontal[0], horizontal[1]\n",
    "    top, bottom = vertical[1], vertical[0]\n",
    "    top = max_height - top\n",
    "    bottom = max_width - bottom\n",
    "    crop = (0, slice(top, bottom), slice(left, right))\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    ax[0].imshow(inp_image[0][crop].cpu(), vmin=vmin, vmax=vmax)\n",
    "    ax[0].set_title(\"Original noisy image\")\n",
    "    ax[1].imshow(noisy[0][crop], vmin=vmin, vmax=vmax)\n",
    "    ax[1].set_title(\"Generated noisy image\")\n",
    "    ax[2].imshow(denoised[0][crop], vmin=vmin, vmax=vmax)\n",
    "    ax[2].set_title(\"Denoised image\")\n",
    "    plt.show()\n",
    "\n",
    "interactive_output_widget = interactive_output(\n",
    "    plot_crop,\n",
    "    {\n",
    "        \"horizontal\": horizontal_widget,\n",
    "        \"vertical\": vertical_widget,\n",
    "    },\n",
    ")\n",
    "\n",
    "sliders = widgets.HBox([horizontal_widget, vertical_widget])\n",
    "\n",
    "layout = widgets.VBox([sliders, interactive_output_widget])\n",
    "\n",
    "display(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spatial correlation of the generated noise can be compared to that of the real noise to get an idea of how accurate the model is. Since we have the denoised version of the generated image, we can get a noise sample by just subtracting it from the noisy versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_noise = low_snr[8, 0, 800:, 800:]\n",
    "generated_noise = noisy[0, 0] - denoised[0, 0]\n",
    "\n",
    "real_ac = utils.autocorrelation(real_noise, max_lag=25)\n",
    "generated_ac = utils.autocorrelation(generated_noise, max_lag=25)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ac1 = ax[0].imshow(real_ac, cmap=\"seismic\", vmin=-1, vmax=1)\n",
    "ax[0].set_title(\"Autocorrelation of real noise\")\n",
    "ax[0].set_xlabel(\"Horizontal lag\")\n",
    "ax[0].set_ylabel(\"Vertical lag\")\n",
    "ac2 = ax[1].imshow(generated_ac, cmap=\"seismic\", vmin=-1, vmax=1)\n",
    "ax[1].set_title(\"Autocorrelation of generated noise\")\n",
    "ax[1].set_xlabel(\"Horizontal lag\")\n",
    "ax[1].set_ylabel(\"Vertical lag\")\n",
    "\n",
    "fig.colorbar(ac2, fraction=0.045)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating new images\n",
    "\n",
    "This time, we'll take a sample from the VAE's prior. This will be a latent variable containing information about a brand new signal. The signal decoder will take that latent variable and convert it into a clean image. The AR decoder will take the latent variable and create an image with the same clean image plus noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 3.1.\n",
    "\n",
    "Set the `n_imgs` variable below to decide how many images to generate. If you set it too high you'll get an out-of-memory error, but don't worry, just restart the kernel and run again with a lower value.\n",
    "\n",
    "Explore the images you generated in the second cell below. Look at the differences between them to see what aspects of the signal the model has learned to generate.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_imgs = ... # Insert an integer here\n",
    "generations = hub.sample_prior(n_imgs=n_imgs)\n",
    "new_denoised = generations[\"s\"].cpu()\n",
    "new_noisy = generations[\"x\"].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d694a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_imgs = 5 # Insert an integer here\n",
    "generations = hub.sample_prior(n_imgs=n_imgs)\n",
    "new_denoised = generations[\"s\"].cpu()\n",
    "new_noisy = generations[\"x\"].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = np.percentile(new_noisy.cpu(), 1)\n",
    "vmax = np.percentile(new_noisy.cpu(), 99)\n",
    "\n",
    "num_images = new_noisy.shape[0]\n",
    "num_channels = new_noisy.shape[1]\n",
    "max_height = new_noisy.shape[2]\n",
    "max_width = new_noisy.shape[3]\n",
    "\n",
    "index_slider = widgets.BoundedIntText(\n",
    "    description=\"Image index: \", min=0, max=num_images, step=1, value=0\n",
    ")\n",
    "\n",
    "vertical_widget = widgets.IntRangeSlider(\n",
    "    description=\"Vertical crop\",\n",
    "    min=0,\n",
    "    max=max_height,\n",
    "    step=1,\n",
    "    value=[0, max_height],\n",
    "    orientation=\"vertical\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(margin='0 0 0 30px')\n",
    ")\n",
    "horizontal_widget = widgets.IntRangeSlider(\n",
    "    description=\"Horizontal crop\",\n",
    "    min=0,\n",
    "    max=max_width,\n",
    "    step=1,\n",
    "    value=[0, max_width],\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "### Explore slices of the data here\n",
    "def plot_crop(image_index, horizontal, vertical):\n",
    "    left, right = horizontal[0], horizontal[1]\n",
    "    top, bottom = vertical[1], vertical[0]\n",
    "    top = max_height - top\n",
    "    bottom = max_width - bottom\n",
    "    crop = (0, slice(top, bottom), slice(left, right))\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    ax[0].imshow(new_noisy[image_index][crop], vmin=vmin, vmax=vmax)\n",
    "    ax[0].set_title(\"Generated noisy image\")\n",
    "    ax[1].imshow(new_denoised[image_index][crop], vmin=vmin, vmax=vmax)\n",
    "    ax[1].set_title(\"Generated clean image\")\n",
    "    plt.show()\n",
    "\n",
    "interactive_output_widget = interactive_output(\n",
    "    plot_crop,\n",
    "    {\n",
    "        \"image_index\": index_slider,\n",
    "        \"horizontal\": horizontal_widget,\n",
    "        \"vertical\": vertical_widget,\n",
    "    },\n",
    ")\n",
    "\n",
    "sliders = widgets.HBox([horizontal_widget, vertical_widget])\n",
    "slide_and_index = widgets.VBox([index_slider, sliders])\n",
    "\n",
    "layout = widgets.VBox([slide_and_index, interactive_output_widget])\n",
    "\n",
    "display(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "### Checkpoint 3\n",
    "\n",
    "In this notebook, we saw how the model you trained in the first notebook has learned to describe the data. We first added a new sample of noise to an existing noisy image. We then generated a clean image that looks like it could be from the training data but doesn't actually exist. <br>\n",
    "You can now optionally return to section 3.1 to load a model that's been trained for much longer, otherwise, you've finished this module on COSDD.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
