{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2e5d27",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "solution",
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "# ruff: noqa: F811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercise: Training COSDD\n",
    "\n",
    "In this section, we will train a COSDD model to remove row correlated and signal-dependent imaging noise. \n",
    "You will load noisy data and examine the noise for spatial correlation, then initialise a model and monitor its training.\n",
    "Finally, you'll use the model to denoise the data.\n",
    "\n",
    "COSDD is a Ladder VAE with an autoregressive decoder -- a type of deep generative model. Deep generative models are trained with the objective of capturing all the structures and characteristics present in a dataset, i.e., modelling the dataset. In our case the dataset will be a collection of noisy microscopy images. \n",
    "\n",
    "When COSDD is trained to model noisy images, it exploits differences between the structure of imaging noise and the structure of the clean signal to separate them, capturing each with different components of the model. Specifically, the noise will be captured by the autoregressive decoder and the signal will be captured by the VAE's latent variables. We can then feed an image into the model and sample a latent variable that will describe the image's clean signal content. This latent variable is then fed through a second network, which was trained alongside the main VAE, to reveal an estimate of the denoised image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "Set your python kernel to <code>05_image_restoration</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.plugins.environments import LightningEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ipywidgets import interactive_output\n",
    "import ipywidgets as widgets\n",
    "import yaml\n",
    "\n",
    "from COSDD import utils\n",
    "from COSDD.models.get_models import get_models\n",
    "from COSDD.models.hub import Hub\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Load the data\n",
    "\n",
    "In this example, we will be using the Mito Confocal dataset provided by: \n",
    "Hagen, G.M., Bendesky, J., Machado, R., Nguyen, T.A., Kumar, T. and Ventura, J., 2021. Fluorescence microscopy datasets for training deep neural networks. GigaScience, 10(5), p.giab032."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "This data contains noise that is correlated along rows. We'll have a closer look at that later.\n",
    "For now, let's load it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 1.1.\n",
    "\n",
    "The low signal-to-noise ratio data that we will be denoising has been downloaded and stored in the `../data` directory as `mito-confocal-lowsnr.tif`. We will load it in the following cell using `utils.load_data`. This requires four arguments that are described below. `paths`, `axes` and `n_dimensions` have already been entered. \n",
    "\n",
    "Enter the file name for `patterns`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "`paths` (str): Path to the directory the training data is stored in. Can be a list of strings if using more than one directory.\n",
    "\n",
    "`patterns` (str): glob pattern to identify files within `paths` that will be used as training data.\n",
    "\n",
    "`axes` (str): (S(ample) | C(hannel) | T(ime) | Z | Y | X). The meaning of each axis in the loaded data, e.g., for a stack of images \"SYX\". \n",
    "\n",
    "`n_dimensions` (int): Number of spatial dimensions of your data, i.e, 1 for time series, 2 for images, 3 for volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "paths = \"../data\"\n",
    "patterns = ... # Enter the data's file name here\n",
    "axes = \"SYX\"\n",
    "n_dimensions = 2\n",
    "low_snr, original_sizes = utils.load_data(\n",
    "    paths=paths, patterns=patterns, axes=axes, n_dimensions=n_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58788c05",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "paths = \"../data\"\n",
    "patterns = \"mito-confocal-lowsnr.tif\"\n",
    "axes = \"SYX\"\n",
    "n_dimensions = 2\n",
    "low_snr, original_sizes = utils.load_data(\n",
    "    paths=paths, patterns=patterns, axes=axes, n_dimensions=n_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 1.2.\n",
    "\n",
    "The data is held in the `low_snr` variable. \n",
    "\n",
    "Check the shape and data type using `.shape` and `.dtype`.\n",
    "\n",
    "The shape should be of the format: (Number of images, Number of channels, Height, Width), and the data type should be float32.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "print(f\"Noisy data size: {low_snr...}\")  # Replace ... with the correct attribute\n",
    "print(f\"Noisy data dtype: {low_snr...}\")  # Replace ... with the correct attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2498a859",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "print(f\"Noisy data size: {low_snr.shape}\")\n",
    "print(f\"Noisy data dtype: {low_snr.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Examine spatial correlation of the noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "COSDD can be applied to noise that is correlated along rows or columns of pixels (or not spatially correlated at all).\n",
    "However, it cannot be applied to noise that is correlated along rows *and* columns of pixels.\n",
    "Noise2Void is designed only for noise that is not spatially correlated at all.\n",
    "\n",
    "When we say that the noise is spatially correlated, we mean that knowing the value of the noise in one pixel tells us something about the noise in other (usually nearby) pixels.\n",
    "Specifically, positive correlatation between two pixels tells us that if the intensity of the noise value in one pixel is high, the intensity of the noise value in the other pixel is likely to be high.\n",
    "Similarly, if one is low, the other is likely to be low.\n",
    "Negative correlation between pixels means that a low noise intensity in one pixel is more likely if the intensity in the other is high, and vice versa.\n",
    "\n",
    "To examine an image's spatial correlation, we can create an autocorrelation plot. \n",
    "The plot will have two axes, horizontal lag and vertical lag, and it tells us what the correlation between a pair of pixels separated by a given horizontal and vertical lag is.\n",
    "For example, if the square at a horizontal lag of 3 and a vertical lag of 6 is red, it means that if we picked any pixel in the image, then counted 3 pixels to the right and 6 pixels down, this pair of pixels is positively correlated.\n",
    "Correlation is symmetric, so the same is true if we counted left or up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "### Question 2.1.\n",
    "\n",
    "Below are three examples of noise. Beneath each is an autocorrelation plot showing how they are spatially correlated.\n",
    "Identify which noise examples could be removed by:\n",
    "(a) COSDD\n",
    "(b) Noise2Void\n",
    "(c) neither\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"resources/ac-question.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "1: COSDD and Noise2Void\n",
    "2: COSDD\n",
    "3: Neither"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 2.1.\n",
    "\n",
    "Now we will create an autocorrelation plot of the noise in the data we loaded.\n",
    "To do this, we need a sample of pure noise.\n",
    "This can be a dark patch of the background in `low_snr`. \n",
    "\n",
    "Run the cell below to start looking at the data.\n",
    "Adjust the sliders for `Image index`, `Top`, `Bottom`, `Left` and `Right` to explore crops of the data and identify a suitable background patch.\n",
    "When decided, click `calculate autocorrelation`.\n",
    "Your autocorrelation plot should report only horizontal correlation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vmin = np.percentile(low_snr, 1)\n",
    "vmax = np.percentile(low_snr, 99)\n",
    "\n",
    "num_images = low_snr.shape[0]\n",
    "num_channels = low_snr.shape[1]\n",
    "max_height = low_snr.shape[2]\n",
    "max_width = low_snr.shape[3]\n",
    "\n",
    "index_slider = widgets.BoundedIntText(\n",
    "    description=\"Image index: \", min=0, max=num_images, step=1, value=0\n",
    ")\n",
    "vertical_widget = widgets.IntRangeSlider(\n",
    "    description=\"Vertical crop\",\n",
    "    min=0,\n",
    "    max=max_height,\n",
    "    step=1,\n",
    "    value=[0, max_height],\n",
    "    orientation=\"vertical\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(margin='0 0 0 30px')\n",
    ")\n",
    "horizontal_widget = widgets.IntRangeSlider(\n",
    "    description=\"Horizontal crop\",\n",
    "    min=0,\n",
    "    max=max_width,\n",
    "    step=1,\n",
    "    value=[0, max_width],\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "autocorr_button = widgets.ToggleButton(description=\"Calculate autocorrelation\", layout=widgets.Layout(width='200px'))\n",
    "\n",
    "\n",
    "def reset_toggle(*args):\n",
    "    autocorr_button.value = False\n",
    "\n",
    "\n",
    "index_slider.observe(reset_toggle, \"value\")\n",
    "vertical_widget.observe(reset_toggle, \"value\")\n",
    "horizontal_widget.observe(reset_toggle, \"value\")\n",
    "\n",
    "\n",
    "### Explore slices of the data here\n",
    "def plot_crop(image_index, horizontal, vertical, plot_ac=False):\n",
    "    left, right = horizontal[0], horizontal[1]\n",
    "    top, bottom = vertical[1], vertical[0]\n",
    "    top = max_height - top\n",
    "    bottom = max_width - bottom\n",
    "    crop = (image_index, 0, slice(top, bottom), slice(left, right))\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    ax[0].imshow(low_snr[crop], vmin=vmin, vmax=vmax)\n",
    "    if plot_ac:\n",
    "        max_lag = min(min(25, bottom - top), min(25, right - left))\n",
    "        noise_ac = utils.autocorrelation(low_snr[crop], max_lag=max_lag)\n",
    "        ac = ax[1].imshow(noise_ac, cmap=\"seismic\", vmin=-1, vmax=1)\n",
    "        fig.colorbar(ac, fraction=0.045)\n",
    "        ax[1].set_title(\"Autocorrelation plot\")\n",
    "        ax[1].set_xlabel(\"Horizontal lag\")\n",
    "        ax[1].set_ylabel(\"Vertical lag\")\n",
    "    else:\n",
    "        ax[1].imshow(np.zeros_like(low_snr[crop]), cmap=\"seismic\", vmin=-1, vmax=1)\n",
    "        ax[1].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "interactive_output_widget = interactive_output(\n",
    "    plot_crop,\n",
    "    {\n",
    "        \"image_index\": index_slider,\n",
    "        \"horizontal\": horizontal_widget,\n",
    "        \"vertical\": vertical_widget,\n",
    "        \"plot_ac\": autocorr_button,\n",
    "    },\n",
    ")\n",
    "\n",
    "index_and_ac = widgets.HBox([index_slider, autocorr_button])\n",
    "sliders = widgets.HBox([horizontal_widget, vertical_widget])\n",
    "slide_and_index = widgets.VBox([index_and_ac, sliders])\n",
    "\n",
    "layout = widgets.VBox([slide_and_index, interactive_output_widget])\n",
    "\n",
    "display(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the autocorrelation plot, all of the squares should be white, except for the top row. The autocorrelation of the square at (0, 0) will always be 1.0 because a pixel's value will always be perfectly correlated with itself. We define this type of noise as correlated along the x axis.\n",
    "\n",
    "This is the type of noise that COSDD is designed to remove.\n",
    "Note that COSDD would still work if the data contained spatially *un*correlated noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Create dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that we're familar with our data, we can get it into a dataloader ready to train a denoiser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 3.1.\n",
    "\n",
    "We will use `utils.DataModule` to prepare the dataloaders. This has four arguments that are described below.\n",
    "Three have already been set.\n",
    "\n",
    "Set `train_split` such that 90% of the images will be used as a training set and 10% used as a validation set.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "`batch_size` (int): Number of images passed through the network at a time. \n",
    "`n_grad_batches` (int): Number of batches to pass through the network before updating parameters.\n",
    "`crop_size` (tuple(int)): The size of randomly cropped patches. Should be less than the dimensions of your images.\n",
    "`train_split` (0 < float < 1): Fraction of images to be used in the training set, with the remainder used for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "real_batch_size = 4\n",
    "n_grad_batches = 4\n",
    "print(f\"Effective batch size: {real_batch_size * n_grad_batches}\")\n",
    "crop_size = (256, 256)\n",
    "train_split = ...  # Enter a training split here\n",
    "\n",
    "datamodule = utils.DataModule(\n",
    "    low_snr=low_snr,\n",
    "    batch_size=real_batch_size,\n",
    "    rand_crop_size=crop_size,\n",
    "    train_split=train_split,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff71ecb1",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "real_batch_size = 4\n",
    "n_grad_batches = 4\n",
    "print(f\"Effective batch size: {real_batch_size * n_grad_batches}\")\n",
    "crop_size = (256, 256)\n",
    "train_split = 0.9\n",
    "\n",
    "datamodule = utils.DataModule(\n",
    "    low_snr=low_snr,\n",
    "    batch_size=real_batch_size,\n",
    "    rand_crop_size=crop_size,\n",
    "    train_split=train_split,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "### Checkpoint 1\n",
    "With our data ready, we can use it to train a COSDD model for denoising.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"resources/explainer.png\"/>\n",
    "\n",
    "COSDD is a Variational Autoencoder (solid arrows) trained to model the distribution of noisy images $\\mathbf{x}$. \n",
    "\n",
    "a) \n",
    "The autoregressive (AR) decoder models the noise component of the images, while the latent variable models only the clean signal component $\\mathbf{s}$.\n",
    "In a second step (dashed arrows), the *signal decoder* is trained to map latent variables into image space, producing an estimate of the signal underlying $\\mathbf{x}$.\n",
    "\n",
    "b)\n",
    "To ensure that the decoder models only the imaging noise and the latent variables capture only the signal, the AR decoder's receptive field is modified.\n",
    "In a full AR receptive field, each output pixel (red) is a function of all input pixels located above and to the left (blue). In our decoder's row-based AR receptive field, each output pixel is a function of input pixels located in the same row, which corresponds to the row-correlated structure of imaging noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 4.1.\n",
    "\n",
    "The model we will train to denoise consists of four modules.\n",
    "Each has it's own hyper-parameters.\n",
    "Most have been set to defaults for a small model.\n",
    "\n",
    "There are two hyperparameters that need to be set:\n",
    "1) `noise_direction`. This tells the model which axis our noise is correlated along. It needs to be set to either `x`, `y` or `z`. Look at the autocorrelation plot to set the correct value.\n",
    "2) `use_direct_denoiser`. Setting this to `True` will slightly slow down training but, once the model is trained, will massively speed up denoising. Set to your preference of `True` or `False`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "`lvae` The ladder variational autoencoder that will output latent variables.\n",
    "* `s_code_channels` (int): Number of channels in outputted latent variable.\n",
    "* `n_layers` (int): Number of levels in the ladder vae.\n",
    "\n",
    "`ar_decoder` The autoregressive decoder that will decode latent variables into a distribution over the input.\n",
    "* `noise_direction` (str): Axis along which noise is correlated: `\"x\"`, `\"y\"` or `\"z\"`. This needs to match the orientation of the noise structures we revealed in the autocorrelation plot in Task 1.2.\n",
    "* `n_gaussians` (int): Number of components in Gaussian mixture used to model data.\n",
    "\n",
    "`direct_denoiser` The U-Net that can optionally be trained to predict the MMSE or MMAE of the denoised images. This will slow training slightly but massively speed up inference and is worthwile if you have an inference dataset in the gigabytes. See [this paper](https://arxiv.org/abs/2310.18116). Enable or disable the direct denoiser by setting `use_direct_denoiser` to `True` or `False`.\n",
    "* `loss_fn` (str): Whether to use `\"L1\"` or `\"MSE\"` loss function to predict either the mean or pixel-wise median of denoised images respectively.\n",
    "\n",
    "`hub` The hub that will unify and train the above modules.\n",
    "* `gradient_checkpoints` (bool): Whether to use gradient checkpointing during training. This reduces memory consumption but increases training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "s_code_channels = 64\n",
    "n_layers = 6\n",
    "noise_direction = ...  # \n",
    "n_gaussians = 8\n",
    "use_direct_denoiser = ...  # \n",
    "dd_loss_fn = \"MSE\"\n",
    "graident_checkpoints = False\n",
    "\n",
    "config = {\n",
    "    \"data\": {\n",
    "        \"number-dimensions\": n_dimensions,\n",
    "    },\n",
    "    \"train-parameters\": {\n",
    "        \"number-grad-batches\": n_grad_batches,\n",
    "        \"use-direct-denoiser\": use_direct_denoiser,\n",
    "        \"direct-denoiser-loss\": dd_loss_fn,\n",
    "        \"crop-size\": crop_size,\n",
    "    },\n",
    "    \"hyper-parameters\": {\n",
    "        \"s-code-channels\": s_code_channels,\n",
    "        \"number-layers\": n_layers,\n",
    "        \"number-gaussians\": n_gaussians,\n",
    "        \"noise-direction\": noise_direction,\n",
    "    },\n",
    "}\n",
    "config = utils.get_defaults(config)\n",
    "\n",
    "lvae, ar_decoder, s_decoder, direct_denoiser = get_models(config, n_channels=low_snr.shape[1])\n",
    "\n",
    "data_mean = low_snr.mean()\n",
    "data_std = low_snr.std()\n",
    "hub = Hub(\n",
    "    vae=lvae,\n",
    "    ar_decoder=ar_decoder,\n",
    "    s_decoder=s_decoder,\n",
    "    direct_denoiser=direct_denoiser,\n",
    "    data_mean=data_mean,\n",
    "    data_std=data_std,\n",
    "    n_grad_batches=n_grad_batches,\n",
    "    checkpointed=graident_checkpoints,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601e4e9",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "s_code_channels = 64\n",
    "n_layers = 6\n",
    "noise_direction = \"x\"\n",
    "n_gaussians = 8\n",
    "use_direct_denoiser = True\n",
    "dd_loss_fn = \"MSE\"\n",
    "graident_checkpoints = False\n",
    "\n",
    "config = {\n",
    "    \"data\": {\n",
    "        \"number-dimensions\": n_dimensions,\n",
    "    },\n",
    "    \"train-parameters\": {\n",
    "        \"number-grad-batches\": n_grad_batches,\n",
    "        \"use-direct-denoiser\": use_direct_denoiser,\n",
    "        \"direct-denoiser-loss\": dd_loss_fn,\n",
    "        \"crop-size\": crop_size,\n",
    "    },\n",
    "    \"hyper-parameters\": {\n",
    "        \"s-code-channels\": s_code_channels,\n",
    "        \"number-layers\": n_layers,\n",
    "        \"number-gaussians\": n_gaussians,\n",
    "        \"noise-direction\": noise_direction,\n",
    "    },\n",
    "}\n",
    "config = utils.get_defaults(config)\n",
    "\n",
    "lvae, ar_decoder, s_decoder, direct_denoiser = get_models(config, n_channels=low_snr.shape[1])\n",
    "\n",
    "data_mean = low_snr.mean()\n",
    "data_std = low_snr.std()\n",
    "hub = Hub(\n",
    "    vae=lvae,\n",
    "    ar_decoder=ar_decoder,\n",
    "    s_decoder=s_decoder,\n",
    "    direct_denoiser=direct_denoiser,\n",
    "    data_mean=data_mean,\n",
    "    data_std=data_std,\n",
    "    n_grad_batches=n_grad_batches,\n",
    "    checkpointed=graident_checkpoints,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 5.1.\n",
    "\n",
    "Open Tensorboard to monitor training. (See Task 3 of 01_CARE).\n",
    "Choose `03_COSDD/checkpoints` for the folder.\n",
    "In there, you'll see the training logs of a model that was trained for about 4 hours.\n",
    "\n",
    "Unlike CARE, this model has more than one loss curve.\n",
    "The cell below describes how to interpret each one.\n",
    "\n",
    "We're going to train our model for only 15 minutes, but we should see the training logs start to follow this previous model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Tensorboard metrics\n",
    "\n",
    "In the SCALARS tab, there will be 4 metrics to track (5 if direct denoiser is enabled). These are:\n",
    "1. `kl_div` The Kullback-Leibler divergence between the VAE's approximate posterior and its prior. This can be thought of as a measure of how much information about the input image is going into the VAE's latent variables. We want information about the input's underlying clean signal to go into the latent variables, so this metric shouldn't go all the way to zero. Instead, it can typically go either up or down during training before plateauing.\n",
    "2. `nll` The negative log-likelihood of the AR decoder's predicted distribution given the input data. This is how accurately the AR decoder is able to predict the input. This value can go below zero and should decrease throughout training before plateauing.\n",
    "3. `elbo` The Evidence Lower Bound, which is the total loss of the main VAE. This is the sum of the kl and reconstruction loss and should decrease throughout training before plateauing.\n",
    "4. `sd_loss` The mean squared error between the noisy image and the image predicted by the signal decoder. This metric should steadily decrease towards zero without ever reaching it. Sometimes the loss will not go down for the first few epochs because its input (produced by the VAE) is rapidly changing. This is ok and the loss should start to decrease when the VAE stabilises. \n",
    "5. `dd_loss` The mean squared error between the output of the direct denoiser and the clean images predicted by the signal decoder. This will only be present if `use_direct_denoiser` is set to `True`. The metric should steadily decrease towards zero without ever reaching it, but may be unstable at the start of training as its targets (produced by the signal decoder) are rapidly changing.\n",
    "\n",
    "There will also be an IMAGES tab. This shows noisy input images from the validation set and some outputs. These will be two randomly sampled denoised images (sample 1 and sample 2), the average of ten denoised images (mmse) and if the direct denoiser is enabled, its output (direct estimate).\n",
    "\n",
    "If noise has not been fully removed from the output images, try increasing `n_gaussians` argument of the AR decoder. This will give it more flexibility to model complex noise characteristics. However, setting the value too high can lead to unstable training. Typically, values from 3 to 5 work best.\n",
    "\n",
    "Note that the trainer is set to train for only 10 minutes in this example. Remove the line with `max_time` to train fully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 5.2.\n",
    "\n",
    "In the following cell, set a sensible `model_name`. You will use this to recall the trained model later.\n",
    "\n",
    "Run the cell after to start training.\n",
    "\n",
    "The `max_time` parameter in the cell below means we'll only train the model for 15 minutes, just to get idea of what to expect. In the future, to remove the time restriction, the `max_time` parameter can be set to `None`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "`model_name` (str): Should be set to something appropriate so that the trained parameters can be used later for inference.\n",
    "\n",
    "`max_epochs` (int): The number of training epochs.\n",
    "\n",
    "`patience` (int): If the validation loss has plateaued for this many epochs, training will stop.\n",
    "\n",
    "`max_time` (str): Maximum time to train for. Must be of form \"DD:HH:MM:SS\", or just `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "model_name = ...  # Enter a model name here\n",
    "max_epochs = 250\n",
    "patience = 50\n",
    "max_time = \"00:00:15:00\"\n",
    "gpu_idx = [0]\n",
    "\n",
    "checkpoint_path = os.path.join(\"checkpoints\", model_name)\n",
    "logger = TensorBoardLogger(checkpoint_path)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=gpu_idx,\n",
    "    max_epochs=max_epochs,\n",
    "    max_time=max_time,\n",
    "    callbacks=[EarlyStopping(patience=patience, monitor=\"elbo/val\")],\n",
    "    precision=\"32\",\n",
    "    plugins=[LightningEnvironment()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e353de8",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "model_name = \"mito-confocal\"\n",
    "max_epochs = 250\n",
    "patience = 50\n",
    "max_time = \"00:00:15:00\"\n",
    "gpu_idx = [0]\n",
    "\n",
    "checkpoint_path = os.path.join(\"checkpoints\", model_name)\n",
    "logger = TensorBoardLogger(checkpoint_path)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=gpu_idx,\n",
    "    max_epochs=max_epochs,\n",
    "    max_time=max_time,\n",
    "    callbacks=[EarlyStopping(patience=patience, monitor=\"elbo/val\")],\n",
    "    precision=\"bf16-mixed\",\n",
    "    plugins=[LightningEnvironment()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    trainer.fit(hub, datamodule=datamodule)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"KeyboardInterupt\")\n",
    "finally:\n",
    "    # Save trained model\n",
    "    trainer.save_checkpoint(os.path.join(checkpoint_path, f\"final_model.ckpt\"))\n",
    "    with open(os.path.join(checkpoint_path, 'training-config.yaml'), 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## Checkpoint 2\n",
    "We've now trained a COSDD model to denoise our data. Continue to the next part to use it to get some results.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Load test data\n",
    "The images that we want to denoise are loaded here. These are the same that we used for training, but we'll only load 5 to speed up inference.\n",
    "\n",
    "We'll also get them into a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "paths = \"../data\"\n",
    "patterns = \"mito-confocal-lowsnr.tif\"\n",
    "axes = \"SYX\"\n",
    "n_dimensions = 2\n",
    "test_data, original_sizes = utils.load_data(\n",
    "    paths=paths, patterns=patterns, axes=axes, n_dimensions=n_dimensions\n",
    ")\n",
    "test_data = test_data[:3]\n",
    "print(f\"Test data size: {test_data.size()}\")\n",
    "\n",
    "predict_batch_size = 1\n",
    "\n",
    "predict_set = utils.PredictDataset(low_snr)\n",
    "predict_loader = torch.utils.data.DataLoader(\n",
    "    predict_set,\n",
    "    batch_size=predict_batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Load trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 7.1.\n",
    "\n",
    "Our model was only trained for 10 minutes. This is long enough to get some denoising results, but a model trained for longer would do better. In the cell below, load the trained model by recalling the value you gave for `model_name`. Then procede through the notebook to look at how well it performs. \n",
    "\n",
    "Once you reach the end of the notebook, return to this cell to load a model that has been trained for 3.5 hours by uncommenting line 3, then run the notebook again to see how much difference the extra training time makes. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "model_name = ...  ### Insert the model name here\n",
    "checkpoint_path = os.path.join(\"checkpoints\", model_name)\n",
    "# checkpoint_path = \"checkpoints/mito-confocal-pretrained\" ### Once you reach the bottom of the notebook, return here and uncomment this line to see the pretrained model\n",
    "\n",
    "with open(os.path.join(checkpoint_path, \"training-config.yaml\")) as f:\n",
    "    train_cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "lvae, ar_decoder, s_decoder, direct_denoiser = get_models(train_cfg, low_snr.shape[1])\n",
    "hub = Hub.load_from_checkpoint(\n",
    "    os.path.join(checkpoint_path, \"final_model.ckpt\"),\n",
    "    vae=lvae,\n",
    "    ar_decoder=ar_decoder,\n",
    "    s_decoder=s_decoder,\n",
    "    direct_denoiser=direct_denoiser,\n",
    ")\n",
    "\n",
    "gpu_idx = [0]\n",
    "predictor = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=gpu_idx,\n",
    "    enable_progress_bar=False,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    precision=\"32\",\n",
    "    plugins=[LightningEnvironment()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f220f",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "model_name = \"mito-confocal\"\n",
    "checkpoint_path = os.path.join(\"checkpoints\", model_name)\n",
    "# checkpoint_path = \"checkpoints/mito-confocal-pretrained\" ### Once you reach the bottom of the notebook, return here and uncomment this line to see the pretrained model\n",
    "\n",
    "with open(os.path.join(checkpoint_path, \"training-config.yaml\")) as f:\n",
    "    train_cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "lvae, ar_decoder, s_decoder, direct_denoiser = get_models(train_cfg, low_snr.shape[1])\n",
    "hub = Hub.load_from_checkpoint(\n",
    "    os.path.join(checkpoint_path, \"final_model.ckpt\"),\n",
    "    vae=lvae,\n",
    "    ar_decoder=ar_decoder,\n",
    "    s_decoder=s_decoder,\n",
    "    direct_denoiser=direct_denoiser,\n",
    ")\n",
    "\n",
    "gpu_idx = [0]\n",
    "predictor = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=gpu_idx,\n",
    "    enable_progress_bar=False,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    precision=\"bf16-mixed\",\n",
    "    plugins=[LightningEnvironment()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8. Denoise\n",
    "In this section, we will look at how COSDD does inference. \n",
    "\n",
    "The model denoises images randomly, giving us a different output each time. First, we will compare seven randomly sampled denoised images for the same noisy image. Then, we will produce a single consensus estimate by averaging 100 randomly sampled denoised images. Finally, if the Direct Denoiser was trained in the previous step, we will see how it can be used to estimate this average in a single pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8.1 Random sampling \n",
    "First, we will denoise each image seven times and look at the difference between each estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_direct_denoiser = False\n",
    "n_samples = 7\n",
    "\n",
    "hub.direct_pred = use_direct_denoiser\n",
    "samples = []\n",
    "for _ in tqdm(range(n_samples)):\n",
    "    out = predictor.predict(hub, predict_loader)\n",
    "    out = torch.cat(out, dim=0)\n",
    "    samples.append(out)\n",
    "\n",
    "samples = torch.stack(samples, dim=1).half()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 8.1.\n",
    "\n",
    "Here, we'll look at the original noisy image and the seven random denoised estimates. Use the sliders to look at different images and adjust the crop. \n",
    "\n",
    "Use this section to really explore the results. Compare high intensity reigons to low intensity reigons, zoom in and out and spot the differences between the different samples. \n",
    "\n",
    "The seven sampled denoised images have differences that express the uncertainty involved in this denoising problem.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vmin = np.percentile(test_data, 1)\n",
    "vmax = np.percentile(test_data, 99)\n",
    "\n",
    "num_images = test_data.shape[0]\n",
    "num_channels = test_data.shape[1]\n",
    "max_height = test_data.shape[2]\n",
    "max_width = test_data.shape[3]\n",
    "\n",
    "index_slider = widgets.BoundedIntText(\n",
    "    description=\"Image index: \", min=0, max=num_images, step=1, value=0\n",
    ")\n",
    "vertical_widget = widgets.IntRangeSlider(\n",
    "    description=\"Vertical crop\",\n",
    "    min=0,\n",
    "    max=max_height,\n",
    "    step=1,\n",
    "    value=[0, max_height],\n",
    "    orientation=\"vertical\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(margin='0 0 0 30px')\n",
    ")\n",
    "horizontal_widget = widgets.IntRangeSlider(\n",
    "    description=\"Horizontal crop\",\n",
    "    min=0,\n",
    "    max=max_width,\n",
    "    step=1,\n",
    "    value=[0, max_width],\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "### Explore slices of the data here\n",
    "def plot_crop(image_index, horizontal, vertical):\n",
    "    left, right = horizontal[0], horizontal[1]\n",
    "    top, bottom = vertical[1], vertical[0]\n",
    "    top = max_height - top\n",
    "    bottom = max_width - bottom\n",
    "    crop = (0, slice(top, bottom), slice(left, right))\n",
    "    fig, ax = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    ax[0, 0].imshow(test_data[image_index][crop], vmin=vmin, vmax=vmax)\n",
    "    ax[0, 0].set_title(\"Input\")\n",
    "    for i in range(n_samples):\n",
    "        ax[(i + 1) // 4, (i + 1) % 4].imshow(\n",
    "            samples[image_index][i][crop], vmin=vmin, vmax=vmax\n",
    "        )\n",
    "        ax[(i + 1) // 4, (i + 1) % 4].set_title(f\"Sample {i+1}\")\n",
    "    plt.show()\n",
    "\n",
    "interactive_output_widget = interactive_output(\n",
    "    plot_crop,\n",
    "    {\n",
    "        \"image_index\": index_slider,\n",
    "        \"horizontal\": horizontal_widget,\n",
    "        \"vertical\": vertical_widget,\n",
    "    },\n",
    ")\n",
    "\n",
    "sliders = widgets.HBox([horizontal_widget, vertical_widget])\n",
    "slide_and_index = widgets.VBox([index_slider, sliders])\n",
    "\n",
    "layout = widgets.VBox([slide_and_index, interactive_output_widget])\n",
    "\n",
    "display(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8.2 MMSE estimate\n",
    "\n",
    "In the next cell, we sample many denoised images and average them for the minimum mean square estimate (MMSE). The averaged images will be stored in the `MMSEs` variable, which has the same dimensions as `low_snr`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 8.2.\n",
    "\n",
    "In the next cell, we will sample 100 randomly denoised estimates. \n",
    "Explore their average - the MMSE estimate - to understand the smoothing effect of averaging so many samples.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_direct_denoiser = False\n",
    "n_samples = 100\n",
    "\n",
    "hub.direct_pred = use_direct_denoiser\n",
    "\n",
    "samples = []\n",
    "for _ in tqdm(range(n_samples)):\n",
    "    out = predictor.predict(hub, predict_loader)\n",
    "    out = torch.cat(out, dim=0)\n",
    "    samples.append(out)\n",
    "\n",
    "samples = torch.stack(samples, dim=1).half()\n",
    "MMSEs = torch.mean(samples, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vmin = np.percentile(test_data, 1)\n",
    "vmax = np.percentile(test_data, 99)\n",
    "\n",
    "num_images = test_data.shape[0]\n",
    "num_channels = test_data.shape[1]\n",
    "max_height = test_data.shape[2]\n",
    "max_width = test_data.shape[3]\n",
    "\n",
    "index_slider = widgets.BoundedIntText(\n",
    "    description=\"Image index: \", min=0, max=num_images, step=1, value=0\n",
    ")\n",
    "vertical_widget = widgets.IntRangeSlider(\n",
    "    description=\"Vertical crop\",\n",
    "    min=0,\n",
    "    max=max_height,\n",
    "    step=1,\n",
    "    value=[0, max_height],\n",
    "    orientation=\"vertical\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(margin='0 0 0 30px')\n",
    ")\n",
    "horizontal_widget = widgets.IntRangeSlider(\n",
    "    description=\"Horizontal crop\",\n",
    "    min=0,\n",
    "    max=max_width,\n",
    "    step=1,\n",
    "    value=[0, max_width],\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "### Explore slices of the data here\n",
    "def plot_crop(image_index, horizontal, vertical):\n",
    "    left, right = horizontal[0], horizontal[1]\n",
    "    top, bottom = vertical[1], vertical[0]\n",
    "    top = max_height - top\n",
    "    bottom = max_width - bottom\n",
    "    crop = (0, slice(top, bottom), slice(left, right))\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    ax[0].imshow(test_data[image_index][crop], vmin=vmin, vmax=vmax)\n",
    "    ax[0].set_title(\"Input\")\n",
    "    ax[1].imshow(samples[image_index][0][crop], vmin=vmin, vmax=vmax)\n",
    "    ax[1].set_title(\"Sample\")\n",
    "    ax[2].imshow(MMSEs[image_index][crop], vmin=vmin, vmax=vmax)\n",
    "    ax[2].set_title(\"MMSE\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "interactive_output_widget = interactive_output(\n",
    "    plot_crop,\n",
    "    {\n",
    "        \"image_index\": index_slider,\n",
    "        \"horizontal\": horizontal_widget,\n",
    "        \"vertical\": vertical_widget,\n",
    "    },\n",
    ")\n",
    "\n",
    "sliders = widgets.HBox([horizontal_widget, vertical_widget])\n",
    "slide_and_index = widgets.VBox([index_slider, sliders])\n",
    "\n",
    "layout = widgets.VBox([slide_and_index, interactive_output_widget])\n",
    "\n",
    "display(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The MMSE will usually be closer to the reference than an individual sample and would score a higher PSNR, although it will also be blurrier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8.3 Direct denoising\n",
    "Sampling 100 images and averaging them is a very time consuming. If the direct denoiser was trained in a previous step, it can be used to directly output what the average denoised image would be for a given noisy image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 8.3.\n",
    "\n",
    "Did you enable the direct denoiser before training? If so, set `use_direct_denoiser` to `True` to use the Direct Denoiser for inference. If not, go back to Section 7 to load the pretrained model and return here. \n",
    "\n",
    "Notice how much quicker the direct denoiser is than generating the MMSE results. Visually inspect and explore the results in the same way as before, notice how similar the direct estimate and MMSE estimate are.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "use_direct_denoiser = ...  # Enter True or False here\n",
    "hub.direct_pred = use_direct_denoiser\n",
    "\n",
    "direct = predictor.predict(hub, predict_loader)\n",
    "direct = torch.cat(direct, dim=0).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "use_direct_denoiser = True\n",
    "hub.direct_pred = use_direct_denoiser\n",
    "\n",
    "direct = predictor.predict(hub, predict_loader)\n",
    "direct = torch.cat(direct, dim=0).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vmin = np.percentile(test_data, 1)\n",
    "vmax = np.percentile(test_data, 99)\n",
    "\n",
    "num_images = test_data.shape[0]\n",
    "num_channels = test_data.shape[1]\n",
    "max_height = test_data.shape[2]\n",
    "max_width = test_data.shape[3]\n",
    "\n",
    "index_slider = widgets.BoundedIntText(\n",
    "    description=\"Image index: \", min=0, max=num_images, step=1, value=0\n",
    ")\n",
    "vertical_widget = widgets.IntRangeSlider(\n",
    "    description=\"Vertical crop\",\n",
    "    min=0,\n",
    "    max=max_height,\n",
    "    step=1,\n",
    "    value=[0, max_height],\n",
    "    orientation=\"vertical\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(margin='0 0 0 30px')\n",
    ")\n",
    "horizontal_widget = widgets.IntRangeSlider(\n",
    "    description=\"Horizontal crop\",\n",
    "    min=0,\n",
    "    max=max_width,\n",
    "    step=1,\n",
    "    value=[0, max_width],\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "### Explore slices of the data here\n",
    "def plot_crop(image_index, horizontal, vertical):\n",
    "    left, right = horizontal[0], horizontal[1]\n",
    "    top, bottom = vertical[1], vertical[0]\n",
    "    top = 1024 - top\n",
    "    bottom = 1024 - bottom\n",
    "    crop = (0, slice(top, bottom), slice(left, right))\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    ax[0].imshow(test_data[image_index][crop], vmin=vmin, vmax=vmax)\n",
    "    ax[0].set_title(\"Input\")\n",
    "    ax[1].imshow(direct[image_index][crop], vmin=vmin, vmax=vmax)\n",
    "    ax[1].set_title(\"Direct\")\n",
    "    ax[2].imshow(MMSEs[image_index][crop], vmin=vmin, vmax=vmax)\n",
    "    ax[2].set_title(\"MMSE\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "interactive_output_widget = interactive_output(\n",
    "    plot_crop,\n",
    "    {\n",
    "        \"image_index\": index_slider,\n",
    "        \"horizontal\": horizontal_widget,\n",
    "        \"vertical\": vertical_widget,\n",
    "    },\n",
    ")\n",
    "\n",
    "sliders = widgets.HBox([horizontal_widget, vertical_widget])\n",
    "slide_and_index = widgets.VBox([index_slider, sliders])\n",
    "\n",
    "layout = widgets.VBox([slide_and_index, interactive_output_widget])\n",
    "\n",
    "display(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9. Incorrect receptive field\n",
    "\n",
    "Earlier, when preparing the model, we told it that the noise was correlated along the x axis. \n",
    "If we had instead told it the noise was correlated along the y axis, denoising would have failed.\n",
    "\n",
    "These images show what that would look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./resources/penicillium_ynm.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## Checkpoint 3\n",
    "\n",
    "We've completed the process of training and applying a COSDD model for denoising, but there's still more it can do. Optionally continue to the bonus notebook, bonus-exercise.ipynb, to see how the model of the data can be used to generate new clean and noisy images.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
