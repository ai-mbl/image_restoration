{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MicroSplit: Semantic Unmixing of Fluorescent Microscopy Data\n",
    "\n",
    "In this notebook, you will work with MicroSplit, a deep learning-based computational multiplexing technique that allows imaging multiple cellular structures within a single fluorescent channel. The method enables imaging more cellular structures, imaging them faster, and at reduced overall light exposure.\n",
    "\n",
    "<p>\n",
    "    <img src=\"imgs/Fig1_a.png\" width=\"800\" />\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    <img src=\"imgs/Fig1_b.png\" width=\"800\" />\n",
    "</p>\n",
    "\n",
    "In more detail, MicroSplit performs the task of ***joint splitting and unsupervised denoising*** in fluorescence microscopy.\n",
    "\n",
    "From a technical perspective, given a noisy image with superimposed labeled structures $X$ (e.g., multiple fluorescently labeled structures imaged in the same channel), the goal is to predict multiple, **unmixed and denoised** images $C_1$, ..., $C_k$, each one corresponding to one of the $k$ different structures. Mathematically: $X = C_1 + C_2 + \\dots + C_k + n$, where $n$ is the noise in $X$. \n",
    "\n",
    "MicroSplit is trained with 2 main objectives (losses):\n",
    "- ***Supervised unmixing*** using target (noisy) unmixed images of the labeled structures.\n",
    "- ***Unsupervised denoising*** using a *Noise Model loss*. \n",
    "\n",
    "MicroSplit's architecture is a slightly modified Variational Auto-Encoder (VAE).\n",
    "Specifically, it implements multiple latent spaces in a hierarchical manner. \n",
    "For this reason the architecture is called Ladder VAE (LVAE).\n",
    "A distinctive feature of the model is the use of an additional trick called Lateral Contextualization (LC). \n",
    "It consists in having additional inputs in the Encoder part which include larger field-of-views (FOVs) of the main superimposed input. \n",
    "This enables the Neural Network to receive more long-range content than the one of a single input patch, hence allowing the extraction\n",
    "of global features which has shown to increase accuracy and consistency of unmixed predictions. \n",
    "\n",
    "<p>\n",
    "    <img src=\"imgs/Fig2.png\" width=\"800\" />\n",
    "</p>\n",
    "\n",
    "***NOTE***: you are now probably wondering how we get a larger FOVs for LC if the input is a full microscopy image. \n",
    "Well... the reality is that in microscopy images are usually pretty large and GPUs are *always* too small (ü•≤). \n",
    "Therefore, we usually work on image **patches** obtained by cropping parts of the image. \n",
    "In this context, LC inputs are simply crops centered on the original one including a larger area of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***References:***\n",
    "- VAE paper: [Kingma et al., Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)\n",
    "- LVAE paper: [S√∏nderby et al, Ladder Variational Autoencoders](https://arxiv.org/abs/1602.02282)\n",
    "- MicroSplit paper: [Ashesh et al., Microùïäplit: Semantic Unmixing of Fluorescent Microscopy Data](https://www.biorxiv.org/content/10.1101/2025.02.10.637323v1)\n",
    "\n",
    "***Additional resources:***\n",
    "- For more information about LC, please check this paper where we first introduced the idea: [ŒºSplit: efficient image decomposition for microscopy data](https://openaccess.thecvf.com/content/ICCV2023/papers/Ashesh_uSplit_Image_Decomposition_for_Fluorescence_Microscopy_ICCV_2023_paper.pdf), which enabled the network to understand the global spatial context around the input patch.\n",
    "- To understand in detail how the joint denoising is performed please check this other work: [denoiSplit: a method for joint microscopy image splitting and unsupervised denoising](https://eccv.ecva.net/virtual/2024/poster/2538).\n",
    "P.S. Web is full of videos and blogposts explaining VAEs and LVAEs... just google it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "Set your python kernel to <code>05_image_restoration</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from careamics.lightning import VAEModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from microsplit_reproducibility.configs.data.custom_dataset_2D import get_data_configs\n",
    "from microsplit_reproducibility.configs.factory import (\n",
    "    create_algorithm_config,\n",
    "    get_likelihood_config,\n",
    "    get_loss_config,\n",
    "    get_model_config,\n",
    "    get_optimizer_config,\n",
    "    get_training_config,\n",
    "    get_lr_scheduler_config,\n",
    ")\n",
    "from microsplit_reproducibility.configs.parameters._base import SplittingParameters\n",
    "from microsplit_reproducibility.datasets import create_train_val_datasets\n",
    "from microsplit_reproducibility.utils.callbacks import get_callbacks\n",
    "from microsplit_reproducibility.utils.io import load_checkpoint_path\n",
    "from microsplit_reproducibility.utils.utils import plot_input_patches, clean_ax\n",
    "from microsplit_reproducibility.notebook_utils.custom_dataset_2D import (\n",
    "    get_unnormalized_predictions,\n",
    "    get_target,\n",
    "    get_input,\n",
    "    full_frame_evaluation,\n",
    "    show_sampling,\n",
    "    pick_random_patches_with_content,\n",
    ")\n",
    "\n",
    "\n",
    "from utils import (\n",
    "    get_train_val_data,\n",
    "    compute_metrics,\n",
    "    show_metrics,\n",
    "    load_pretrained_model,\n",
    "    STRUCTURE_2_INDEX\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564532e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = Path(\"/mnt/efs/aimbl_2025/data/05_image_restoration/MicroSplit_MBL_2025/\")  # Path to the data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Exercise 1**: Training MicroSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Since MicroSplit is trained in a supervised manner, we must feed:\n",
    "*(i)* input images containing the superimposed structures, and \n",
    "*(ii)* target images/channels, each one showing one label structure separately. \n",
    "\n",
    "Notice that, for simplicity, the mixed input image is here obtained synthetically by overlapping the other two channels (pixel-wise sum).\n",
    "\n",
    "In this exercise, we will use a dataset imaged at the *National Facility for Light Imaging at Human Technopole*.\n",
    "\n",
    "This dataset contains four labeled structures: \n",
    "1. Cell Nuclei,\n",
    "1. Microtubules,\n",
    "1. Nuclear Membrane,\n",
    "1. Centromeres/Kinetocores.\n",
    "\n",
    "Additionally, this dataset offers acquisitions taken with different exposure times `(2, 20, 500 ms)`. \n",
    "Hence, the data is available at various [signal-to-noise ratios](https://en.wikipedia.org/wiki/Signal-to-noise_ratio#:~:text=Signal%2Dto%2Dnoise%20ratio%20(,power%2C%20often%20expressed%20in%20decibels.)) (SNR). \n",
    "Shorter exposure times only allows the collection of fewer photons, leading to higher *Poisson shot noise* and, therefore, a lower SNR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\"><h4><b>Task 1.1.</b></h4>\n",
    "\n",
    "In the following, you will be asked to select:\n",
    "1. The labeled structures to unmix;\n",
    "2. The exposure time (and, thus, the SNR) of the input superimposed images.\n",
    "\n",
    "Observe that:\n",
    "- The more structures to unmix you pick, the more challenging the task becomes. A 2-structures unmixing is always easier than 3 or 4-structures unmixing.\n",
    "- The lower the SNR of the data you will choose to train $\\mathrm{Micro}\\mathbb{S}\\mathrm{plit}$ with, the more challenging the task becomes and the more important will the unsupervised denoising feature of $\\mathrm{Micro}\\mathbb{S}\\mathrm{plit}$ becomes.\n",
    "\n",
    "You can play with these parameters and check MicroSplit performance with different combinations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a03f2",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "# pick structures and exposure time\n",
    "STRUCTURES = [..., ...] # choose among \"Nuclei\", \"Microtubules\", \"NucMembranes\", \"Centromeres\"\n",
    "EXPOSURE_TIME = ... # in ms, choose among 2, 20, 500 (expressed in ms)\n",
    "\n",
    "assert EXPOSURE_TIME in [2, 20, 500], \"Exposure time must be one of [2, 20, 500] ms\"\n",
    "assert all([\n",
    "    s in [\"Nuclei\", \"Microtubules\", \"NucMembranes\", \"Centromeres\"] for s in STRUCTURES\n",
    "]), \"Invalid structure selected. Choose among 'Nuclei', 'Microtubules', 'NucMembranes', 'Centromeres'.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following cell creates data configuration for training, validation and test sets. Each configurarion defines a set of parameters related to data loading, dataset creation and data processing.\n",
    "\n",
    "For more accessibility, configs are automatically generated by the `get_data_configs` wrapper function, which only requires to define:\n",
    "- `image_size` : `tuple[int]` -> the patch size used to train the model.\n",
    "- `num_channels` : `int` -> the number of target structures to unmix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_config, val_data_config, test_data_config = get_data_configs(\n",
    "    image_size=(64, 64),\n",
    "    num_channels=len(STRUCTURES),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Create the train, val, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datapath = ROOT_DIR / f\"data/{EXPOSURE_TIME}ms\"\n",
    "load_data_func = partial(get_train_val_data, structures=STRUCTURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: here we are loading data from disk, creating synthetic inputs, generating patches... this might take a while\n",
    "train_dset, val_dset, test_dset, data_stats = create_train_val_datasets(\n",
    "    datapath=datapath,\n",
    "    train_config=train_data_config,\n",
    "    val_config=val_data_config,\n",
    "    test_config=val_data_config,\n",
    "    load_data_func=load_data_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Create the train and val dataloaders. The test one is not used for training, hence it will be created later on during evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dloader = DataLoader(\n",
    "    train_dset,\n",
    "    batch_size=32,\n",
    "    num_workers=3,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_dloader = DataLoader(\n",
    "    val_dset,\n",
    "    batch_size=32,\n",
    "    num_workers=3,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\"><h4><b>Check some training data patches!</b></h4>\n",
    "\n",
    "***Tip:*** the following functions shows a few samples of the prepared training data. In case you don't like what you see (empty or noisy patches), execute the cell again. Different randomly chosen patches will be shown!</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_input_patches(dataset=train_dset, num_channels=len(STRUCTURES), num_samples=3, patch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9545ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\"><h4><b>Question 1.1.</b></h4>\n",
    "\n",
    "- Can you tell in which parts of the model the different patches shown below are used?\n",
    "- What do the input patches show? Why are there multiple inputs?\n",
    "- Why do we need targets? How do we use such targets? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5427d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\"><h4><b>Question 1.2.</b></h4>\n",
    "\n",
    "Below are 2 examples of superimposed labeled structures with the correspondent ground truths. \n",
    "1. Which one you think it's harder to unmix? Why?\n",
    "2. What are, in your opinion, features of the input data that would make unmixing more difficult? \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p>\n",
    "    <img src=\"imgs/question1.png\" width=\"800\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\"><h2><b>Checkpoint 1: Data Preparation</b></h2>\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2. Setup MicroSplit for training\n",
    "In this section, we create all the configs for the upcoming model initialization and training run. Configs allow to group all the affine parameters in the same place (architecture, training, loss, etc. etc.) and offer automated validation of the input parameters to prevent the user from inputting wrong combinations.\n",
    "\n",
    "Notice that MicroSplit is being implemented in CAREamics library, therefore the API is quite similar to the one you (perhaps) saw for Noise2Void. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\"><h4><b>Task 1.2.</b></h4>\n",
    "\n",
    "In the following, we will initialize all the different configs that are needed to (i) instantiate a working MicroSplit model; (ii) train the model. \n",
    "\n",
    "To facilitate the task, we define all the customizable parameters within the `get_microsplit_parameters` function. This returns the parameters for the current experiment that are later used to automatically define the single configs.\n",
    "\n",
    "Your task here is to:\n",
    "- Understand the meaning of these customizable parameters.\n",
    "- Play with some of them (only the ones indicated by us). \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here's a break down of the customizable parameters:\n",
    "\n",
    "- `algorithm : str` -> the type of training algorithm to use. `denoisplit` does joint splitting and denoising, whereas `musplit` only does splitting.\n",
    "- `loss_type : str` -> the loss used to train the model. `denoisplit_musplit` practically means that the loss used is a combination of the loss used for `denoisplit` algorithm with some elements taken from `musplit` algorithm to improve splitting performance. \n",
    "- `image_size : tuple[int]` -> the patch size used to train the model.\n",
    "- `noise_model_path : str` -> path to a folder containing the pre-trained noise models for the different channels.\n",
    "- `target_channels : int` -> the number of target structures to unmix (similar to `num_channels` seen before).\n",
    "- `multiscale_count : int` -> the total number of inputs (main + LC) to use. For instance, if set to `3`, it means main input + 2 LC inputs are used.\n",
    "- `lr : float` -> learning rate.\n",
    "- `num_epochs : int` -> the maximum number of epochs the model is trained for.\n",
    "- `lr_scheduler_patience : int` -> number of epochs to wait before decreasing the learning rate. Helps get away from local minima during training. Used by callbacks.\n",
    "- `earlystop_patientce : int` -> number of epochs to wait before applying earlystop. Earlystop consists in stopping training process before it reaches the desired number of epoche is a monitored metric (usually the aggregated validation loss) doesn't improve over some amount of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Get pre-trained noise models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NM_PATH = ROOT_DIR / f\"noise_models/{EXPOSURE_TIME}ms\"\n",
    "\n",
    "paths_to_noise_models = [\n",
    "    str(NM_PATH / f\"noise_model_Ch{STRUCTURE_2_INDEX[structure]}.npz\")\n",
    "    for structure in STRUCTURES\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "Set other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc18816b",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "# setting up MicroSplit parametrization\n",
    "experiment_params = SplittingParameters(\n",
    "    algorithm=\"denoisplit\",\n",
    "    loss_type=\"denoisplit_musplit\",\n",
    "    img_size=(64, 64), # this should be consistent with the dataset\n",
    "    target_channels=len(STRUCTURES),\n",
    "    multiscale_count=3,\n",
    "    lr=1e-3,\n",
    "    num_epochs=..., # <- you can modify this\n",
    "    lr_scheduler_patience=..., # <- you can modify this (note: if you want this to work, must be less than num_epochs)\n",
    "    earlystop_patience=..., # <- you can modify this (note: if you want this to work, must be less than num_epochs)\n",
    "    nm_paths=paths_to_noise_models,\n",
    ").model_dump()\n",
    "\n",
    "# add data stats for standardization\n",
    "experiment_params[\"data_stats\"] = data_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setting up training losses and model config (using default parameters)\n",
    "loss_config = get_loss_config(**experiment_params)\n",
    "model_config = get_model_config(**experiment_params)\n",
    "gaussian_lik_config, noise_model_config, nm_lik_config = get_likelihood_config(\n",
    "    **experiment_params\n",
    ")\n",
    "training_config = get_training_config(**experiment_params)\n",
    "\n",
    "# setting up learning rate scheduler and optimizer (using default parameters)\n",
    "lr_scheduler_config = get_lr_scheduler_config(**experiment_params)\n",
    "optimizer_config = get_optimizer_config(**experiment_params)\n",
    "\n",
    "# finally, assemble the full set of experiment configurations...\n",
    "experiment_config = create_algorithm_config(\n",
    "    algorithm=experiment_params[\"algorithm\"],\n",
    "    loss_config=loss_config,\n",
    "    model_config=model_config,\n",
    "    gaussian_lik_config=gaussian_lik_config,\n",
    "    nm_config=noise_model_config,\n",
    "    nm_lik_config=nm_lik_config,\n",
    "    lr_scheduler_config=lr_scheduler_config,\n",
    "    optimizer_config=optimizer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = VAEModule(algorithm_config=experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.3. Train MicroSplit model\n",
    "\n",
    "In this section we will train out MicroSplit model using `lightning`. We have manually set the time limit to 20 minutes. This limit can be modified with the `max_time` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the Trainer\n",
    "trainer = Trainer(\n",
    "    max_time=\"00:00:25:00\", # this is roughly the time to train for 3 epochs \n",
    "    max_epochs=training_config.num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    enable_progress_bar=True,\n",
    "    callbacks=get_callbacks(\"./checkpoints/\"),\n",
    "    precision=training_config.precision,\n",
    "    gradient_clip_val=training_config.gradient_clip_val,\n",
    "    gradient_clip_algorithm=training_config.gradient_clip_algorithm,\n",
    "    logger=TensorBoardLogger(\"tb_logs\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start the training\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=train_dloader,\n",
    "    val_dataloaders=val_dloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa1a4bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "**NOTE**: each training epoch should take approximately 7 minutes on our GPUs.\n",
    "For the sake of time you can stop training after the 2nd or 3rd epoch... \n",
    "Results will not be as good, but you will still be able to evaluate the model and see how it works.\n",
    "For reference, in our experiments we usually train MicroSplit for 50 or 100 epochs on similarly sized datasets,\n",
    "which takes approximately between 6 and 12 hours on a single GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><h5><b>Task 1.3: Visualize losses and metrics using Tensorboard</b></h5>\n",
    "\n",
    "Open Tensorboard in VS Code to monitor training.\n",
    "\n",
    "You already did it for the 01_CARE exercise so you should know how to do it!\n",
    "However, in case you need a reminder, here are the steps to follow:\n",
    "\n",
    "1) Open the extensions panel in VS Code.\n",
    "2) Search Tensorboard and install the extension published by Microsoft.\n",
    "3) Start training. Run the cell below to begin training the model and generating logs.\n",
    "3) Once training is started. Open the command palette (ctrl+shift+p), search for Python: Launch Tensorboard and hit enter.\n",
    "4) When prompted, select \"Select another folder\" and enter the path to the `04_MicroSplit/tb_logs` directory.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.4 Visualize predictions on validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In order to check that the training process has been successful, we check MicroSplit predictions on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\"><h4><b>Question 1.4.</b></h4>\n",
    "\n",
    "A proper evaluation including prediction on mutliple images and computation of performance metrics will be performed later on the test data.\n",
    "Do you remember what are the limitations of evaluating a model's perfomance on the validation set, instead?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e7740",
   "metadata": {
    "tags": []
   },
   "source": [
    "Before proceeding with the evaluation, let's focus once more on how MicroSplit works.\n",
    "\n",
    "As we mentioned, MicroSplit uses a modified version of the Ladder Variational Autoencoder (LVAE) similarly to DivNoising, HDN, COSDD and other models you encountered during the course. \n",
    "This architecture, given an input patch, enables the generation of multiple outputs. Technically, this happens by sampling multiple different *latent vectors* in the latent space. \n",
    "In mathematical terms we say that \"*MicroSplit is learning a full posterior of possible solutions*\".\n",
    "\n",
    "This is a cool feature that makes our variational models pretty powerful and handy!!!\n",
    "Indeed, averaging multiple samples (predictions) generally allows to get smoother, more consistent predictions (in other terms, it somehow averages out potential \"hallucinations\" of the network). \n",
    "Moreover, by computing the pixel-wise standard deviation over multiple samples (predictions) we can obtain a preliminary estimate of the (data) uncertainty in the model's predictions.\n",
    "\n",
    "In this framework, the parameter `mmse_count : (int)` determines the number of samples (predictions) generated for any given input patch. \n",
    "A larger value allows to get smoother predictions, also limiting recurring issues such as *tiling artefacts*. However, it obviously increases the time and cost of the computation. \n",
    "Generally, a value of `> 5` is enough to get decently smooth predicted frames. For reference, in our papers we often use values of 50 to get the best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54532b",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "MMSE_COUNT = ...\n",
    "\"\"\"The number of MMSE samples to use for the splitting predictions.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reduce the validation dataset to a single structure for quicker prediction\n",
    "val_dset.reduce_data([0])\n",
    "\n",
    "# Get patch predictions for the validation dataset + stitching into full images + de-normalization\n",
    "stitched_predictions, _, _ = get_unnormalized_predictions(\n",
    "    model,\n",
    "    val_dset,\n",
    "    data_key=val_dset._fpath.name,\n",
    "    mmse_count=MMSE_COUNT,\n",
    "    num_workers=3,\n",
    "    grid_size=48,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the target and input from the validation dataset for visualization purposes\n",
    "tar = get_target(val_dset)\n",
    "inp = get_input(val_dset).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frame_idx = 0\n",
    "assert frame_idx < len(stitched_predictions), f\"Frame index {frame_idx} out of bounds. Max index is {len(stitched_predictions) - 1}.\"\n",
    "\n",
    "full_frame_evaluation(stitched_predictions[frame_idx], tar[frame_idx], inp[frame_idx], same_scale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\"><h2><b>Checkpoint 2: Model Training</b></h2>\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Exercise 2**: Evaluating MicroSplit performance\n",
    "\n",
    "So far, you have trained MicroSplit and had a first qualitative evaluation on the validation set. \n",
    "However, at this point of the course you should be familiar with the idea that a proper evaluation should be carried out on a held-out test set,\n",
    "which has not been seen by the model during any part of the training process. In this section we perform the evaluation on the test set, \n",
    "which will include a further qualitative inspection of predicted images and a quantitative evaluation using adequate metrics to measure models' performance\n",
    "\n",
    "Recall that for this task, on a standard GPU, we cannot feed the entire image to $\\mathrm{Micro}\\mathbb{S}\\mathrm{plit}$. \n",
    "Hence, we process smaller chunks of the full image that we so far called **patches**. \n",
    "Usually, at training time these patches are obtained as random crops from the full input images, as random cropping works\n",
    "as a kind of ***data augmentation*** technique. However, at test time we want our predictions to be done on the full images.\n",
    "Hence, we need a more \"organized\" strategy to obtain the patches. An option is to divide the full frames into an ordered grid of patches.\n",
    "In our paper, we call this process ***tiling*** and we call the single crops ***tiles***, to differentiate them from the ones we use for training.\n",
    "\n",
    "A recurrent issue in ***tiled prediction*** is the possible presence of the so-called ***tiling artefacts***, which originate from inconsistencies and mismatches at the borders of neighboring tiles \n",
    "(see (c) - No padding in the figure below). This problem can be alleviated by performing ***padding*** of the input tile, and later discarding the padded area when stitching the predictions. \n",
    "The idea here is to introduce some overlap between neighboring tiles to have a smoother transition between them. Common padding strategies are:\n",
    "- ***Outer padding***: the patch (tile) size used for training (e.g., `(64, 64)`) is padded to a larger size. Then, the padded are is discarded during stitching.\n",
    "- ***Inner padding***: the patch (tile) size used for training (e.g., `(64, 64)`) is used as input for prediction. Then, only the inner part of it is kept during stitiching.\n",
    "\n",
    "In our work we use ***Inner padding*** as it preserves the same field of view the network has seen during training and empirically provides better performance on our task (see (b)).\n",
    "\n",
    "<p>\n",
    "    <img src=\"imgs/tiling.png\" width=\"800\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1. Compute MicroSplit predictions on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\"><h4><b>(Optional) Task 2.1.1: Load checkpoint</b></h4>\n",
    "\n",
    "In case you had any troubles while executing the notebook (disconnection, dead kernel, ...), you can avoid retraining MicroSplit from scratch and load, instead, some of your previous checkpoints.\n",
    "\n",
    "Similarly, if you are not satisfied with your trained model, you can try with the pre-trained one by us. However, we strongly suggest you first try with yours to identify any potential shortcoming, and then you resort back to ours to check how close you got to that.\n",
    "\n",
    "***Note***: unfortunately we cannot provide pre-trained models for all the possible combinations of structures and exposures (if you're curious, there would be 33 combinations of such parameters üòå). Therefore, we provide one pre-trained model for each exposure time with 3 labeled structures to unmix (specifically, microtubules, nuclear membranes and centrosomes).\n",
    "\n",
    "So, run the cells in **Option A** to evaluate the model that you trained, or run the cells in **Option B** to evaluate a model that is pretrained.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "#### **Option A**: load your previous checkpoints\n",
    "\n",
    "In the same subdirectory of the current `exercise.ipynb` notebook, you should see a `checkpoints` folder. This contains the checkpoints of your past training run.\n",
    "\n",
    "***NOTE***: checkpoints are associated to a specific model configuration. Hence, if you changed any parameter in the configuration, you will not be able to load your previous checkpoints.\n",
    "\n",
    "***NOTE***: if there are multiple checkpoints, our function will automatically pick the first found by listing the files in the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_ckpt = load_checkpoint_path(\"./checkpoints\", best=True)\n",
    "print(\"‚úÖ Selected model checkpoint:\", selected_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### End of **Option A**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7dcf58",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "#### **Option B**: load pre-trained checkpoints\n",
    "\n",
    "As we mentioned above, we only have a few pre-trained checkpoints available. For this reason, we will need to reinstantiate configs, datasets, and model to make sure they coincide with one of the pre-trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\"><h5><b>Task: Pick a pre-trained model</b></h5>\n",
    "\n",
    "As we said, you can choose your desired exposure time. Structures will be set to `\"Microtubules\", \"NucMembranes\", \"Centromeres\"`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d698a",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "EXPOSURE_TIME = ... # in ms, choose among 2, 20, 500 (expressed in ms)\n",
    "\n",
    "assert EXPOSURE_TIME in [2, 20, 500], \"Exposure time must be one of [2, 20, 500] ms\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained_ckpt_path = ROOT_DIR / f\"ckpts/{EXPOSURE_TIME}ms\"\n",
    "selected_ckpt = load_checkpoint_path(str(pretrained_ckpt_path), best=True)\n",
    "print(\"‚úÖ Selected model checkpoint:\", selected_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Reinstantiate configs, datasets, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_config, val_data_config, test_data_config = get_data_configs(\n",
    "    image_size=(64, 64),\n",
    "    num_channels=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datapath = ROOT_DIR / f\"data/{EXPOSURE_TIME}ms\"\n",
    "load_data_func = partial(get_train_val_data, structures=[\"Microtubules\", \"NucMembranes\", \"Centromeres\"])\n",
    "\n",
    "train_dset, val_dset, test_dset, data_stats = create_train_val_datasets(\n",
    "    datapath=datapath,\n",
    "    train_config=train_data_config,\n",
    "    val_config=val_data_config,\n",
    "    test_config=val_data_config,\n",
    "    load_data_func=load_data_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get noise models\n",
    "NM_PATH = ROOT_DIR / f\"noise_models/{EXPOSURE_TIME}ms\"\n",
    "paths_to_noise_models = [\n",
    "    str(NM_PATH / f\"noise_model_Ch{STRUCTURE_2_INDEX[structure]}.npz\")\n",
    "    for structure in [\"Microtubules\", \"NucMembranes\", \"Centromeres\"]\n",
    "]\n",
    "\n",
    "# setting up MicroSplit parametrization\n",
    "experiment_params = SplittingParameters(\n",
    "    algorithm=\"denoisplit\",\n",
    "    loss_type=\"denoisplit_musplit\",\n",
    "    img_size=(64, 64),\n",
    "    target_channels=3,\n",
    "    multiscale_count=3,\n",
    "    predict_logvar=\"pixelwise\",\n",
    "    nm_paths=paths_to_noise_models,\n",
    ").model_dump()\n",
    "\n",
    "# add data stats for standardization\n",
    "experiment_params[\"data_stats\"] = data_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setting up training losses and model config (using default parameters)\n",
    "loss_config = get_loss_config(**experiment_params)\n",
    "model_config = get_model_config(**experiment_params)\n",
    "gaussian_lik_config, noise_model_config, nm_lik_config = get_likelihood_config(\n",
    "    **experiment_params\n",
    ")\n",
    "training_config = get_training_config(**experiment_params)\n",
    "\n",
    "# setting up learning rate scheduler and optimizer (using default parameters)\n",
    "lr_scheduler_config = get_lr_scheduler_config(**experiment_params)\n",
    "optimizer_config = get_optimizer_config(**experiment_params)\n",
    "\n",
    "# finally, assemble the full set of experiment configurations...\n",
    "experiment_config = create_algorithm_config(\n",
    "    algorithm=experiment_params[\"algorithm\"],\n",
    "    loss_config=loss_config,\n",
    "    model_config=model_config,\n",
    "    gaussian_lik_config=gaussian_lik_config,\n",
    "    nm_config=noise_model_config,\n",
    "    nm_lik_config=nm_lik_config,\n",
    "    lr_scheduler_config=lr_scheduler_config,\n",
    "    optimizer_config=optimizer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = VAEModule(algorithm_config=experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that checkpoints are loaded, we load these pre-trained weights into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_pretrained_model(model, selected_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada101ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### End of **Option B**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\"><h4><b>Task 2.1.2: Get test set predictions</b></h4>\n",
    "\n",
    "Here we reuse the `get_unnormalized_predictions` you saw before to get the unmixed predicted images for the training set.\n",
    "You will have to:\n",
    "- Set `MMSE_COUNT` parameter, being careful at finding an appropriate trade-off between prediction quality (remember the tiling artefacts we discussed above) and computation time.\n",
    "Given our time contraints, a reasonable range to try is `[2, 10]`.\n",
    "- Set `INNER_TILE_SIZE` parameter, trying different values for inner padding. \n",
    "Also here notice that a smaller `INNER_TILE_SIZE` entails larger padding/overlap between neighboring patches and, hence, more predictions to be done. \n",
    "A reasonable range to try is `[16, 64]`, where `64` means that no padding is done (recall, we used a patch size of `64` during training).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a12d192",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "MMSE_COUNT = ...\n",
    "\"\"\"The number of MMSE samples to use for the splitting predictions.\"\"\"\n",
    "INNER_TILE_SIZE = ...\n",
    "\"\"\"The inner tile size considered for the predictions.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stitched_predictions, _, stitched_stds = (\n",
    "    get_unnormalized_predictions(\n",
    "        model,\n",
    "        test_dset,\n",
    "        data_key=test_dset._fpath.name,\n",
    "        mmse_count=MMSE_COUNT,\n",
    "        grid_size=INNER_TILE_SIZE,\n",
    "        num_workers=3,\n",
    "        batch_size=32,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***NOTE***: you might have seen that the function also returns `stitched_stds`. These are the pixel-wise standard deviations over the `MMSE_COUNT`-many samples for each image (yes, also these have been stitched back to images)!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\"><h2><b>Checkpoint 3: Test set predictions</b></h2>\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.2. Qualitative evaluation of MicroSplit predictions\n",
    "\n",
    "In this section you will provided with tools to interactively inspect the predicted unmixed images from the test set to have a premliminary qualitative evaluation and spot potential issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\"><h4><b>Task 2.2: Look for defects in the obtained predictions</b></h4>\n",
    "\n",
    "Previously we discussed how noise, number of labeled structures, and morphological similarity between label structures can influence the complexity of the unmixing task. Depending on these factors, you might see some defects on your predicted unmixed images. In addition, we mentioned that tiled prediction can cause the so-called tiling artefacts.\n",
    "\n",
    "In this section, your task is to:\n",
    "1. identify these defects (if any).\n",
    "2. determine what is the likely source (e.g., tiling artefact, unmixing failure, ...).\n",
    "\n",
    "You will be provided with functions to visualize (i) full images, (ii) random smaller crops, (iii) custom crops.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the target and input from the test dataset for visualization purposes\n",
    "tar = get_target(test_dset)\n",
    "inp = get_input(test_dset).sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### (i) Full image visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frame_idx = 0 # Change this index to visualize different frames\n",
    "assert frame_idx < len(stitched_predictions), f\"Frame index {frame_idx} out of bounds. Max index is {len(stitched_predictions) - 1}.\"\n",
    "\n",
    "full_frame_evaluation(stitched_predictions[frame_idx], tar[frame_idx], inp[frame_idx], same_scale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "#### (ii) Random crops visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f6bbf1",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Insert here the crop size for visualization ---\n",
    "img_sz = ...\n",
    "# ---\n",
    "\n",
    "rand_locations = pick_random_patches_with_content(tar, img_sz)\n",
    "\n",
    "ncols = 1 + 2 * stitched_predictions.shape[-1]\n",
    "nrows = min(len(rand_locations), 5)\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 3, nrows * 3))\n",
    "\n",
    "for i, (h_start, w_start) in enumerate(rand_locations[:nrows]):\n",
    "    ax[i, 0].imshow(inp[0, h_start : h_start + img_sz, w_start : w_start + img_sz])\n",
    "    for j in range(ncols // 2):\n",
    "        # vmin = stitched_predictions[..., j].min()\n",
    "        # vmax = stitched_predictions[..., j].max()\n",
    "        ax[i, 2 * j + 1].imshow(\n",
    "            tar[0, h_start : h_start + img_sz, w_start : w_start + img_sz, j],\n",
    "            # vmin=vmin,\n",
    "            # vmax=vmax,\n",
    "        )\n",
    "        ax[i, 2 * j + 2].imshow(\n",
    "            stitched_predictions[\n",
    "                0, h_start : h_start + img_sz, w_start : w_start + img_sz, j\n",
    "            ],\n",
    "            # vmin=vmin,\n",
    "            # vmax=vmax,\n",
    "        )\n",
    "\n",
    "ax[0, 0].set_title(\"Primary Input\")\n",
    "for i in range(ncols // 2):  # 2 channel splitting\n",
    "    ax[0, 2 * i + 1].set_title(f\"Target Channel {i+1}\")\n",
    "    ax[0, 2 * i + 2].set_title(f\"Predicted Channel {i+1}\")\n",
    "\n",
    "# reduce the spacing between the subplots\n",
    "plt.subplots_adjust(wspace=0.03, hspace=0.03)\n",
    "clean_ax(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### (iii) Custom crop visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecde823f",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Pick coordinates of upper-left corner and crop size ---\n",
    "y_start = ...\n",
    "x_start = ...\n",
    "crop_size = ...\n",
    "#--------------\n",
    "assert y_start + crop_size <= stitched_predictions.shape[1], f\"y_start + crop_size exceeds image height, which is {stitched_predictions.shape[1]}\"\n",
    "assert x_start + crop_size <= stitched_predictions.shape[2], f\"x_start + crop_size exceeds image width, which is {stitched_predictions.shape[2]}\"\n",
    "\n",
    "ncols = 1 + stitched_predictions.shape[-1]\n",
    "nrows = 2\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 5, nrows * 5), constrained_layout=True)\n",
    "ax[0, 0].imshow(inp[0, y_start : y_start + crop_size, x_start : x_start + crop_size])\n",
    "for i in range(ncols - 1):\n",
    "    # vmin = stitched_predictions[..., i].min()\n",
    "    # vmax = stitched_predictions[..., i].max()\n",
    "    ax[0, i + 1].imshow(\n",
    "        tar[0, y_start : y_start + crop_size, x_start : x_start + crop_size, i],\n",
    "        # vmin=vmin,\n",
    "        # vmax=vmax,\n",
    "    )\n",
    "    ax[1, i + 1].imshow(\n",
    "        stitched_predictions[\n",
    "            0, y_start : y_start + crop_size, x_start : x_start + crop_size, i\n",
    "        ],\n",
    "        # vmin=vmin,\n",
    "        # vmax=vmax,\n",
    "    )\n",
    "    ax[0, i + 1].set_title(f\"Channel {i+1}\", fontsize=15)\n",
    "\n",
    "# disable the axis for ax[1,0]\n",
    "ax[1, 0].axis(\"off\")\n",
    "ax[0, 0].set_title(\"Input\", fontsize=15)\n",
    "# set y labels on the right for ax[0,2]\n",
    "ax[0,ncols-1].yaxis.set_label_position(\"right\")\n",
    "ax[0,ncols-1].set_ylabel(\"Target\", fontsize=15)\n",
    "\n",
    "ax[1,ncols-1].yaxis.set_label_position(\"right\")\n",
    "ax[1,ncols-1].set_ylabel(\"Predicted\", fontsize=15)\n",
    "\n",
    "print(\"Here the crop you selected:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\"><h4><b>Question 2.2.</b></h4>\n",
    "\n",
    "Can you come up with any idea about how to get rid of the current issues in the predictions? \n",
    "Take into account the things we mentioned during the course so far...\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d16ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\"><h4><b>Bonus Question</b></h4>\n",
    "\n",
    "In this and other exercises we spoke of \"tiling artefacts\". These are generally due to a mismatch in the predictions of adjacent tiles/patches. \n",
    "In the context of CNN and, specifically, VAE-based models, can you think about reasons why we have such effect?\n",
    "\n",
    "*Hint1*: for CNN, think about how convolution works at the image borders... <br>\n",
    "*Hint2*: for VAE, reflect on the sampling happening in the latent space...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.3. Quantitative evaluation of MicroSplit predictions\n",
    "\n",
    "In this section you will perform a quantitative evaluation of MicroSplit unmixing performance using the provided function to compute metrics. In image restoration there are several commonly used metrics to quantitatively assess the goodness of a model's predictions. Clearly, different metrics focus on different aspects and provide different insights. Some metrics evaluate the ***pixel-wise similarity*** between images, while some other focus on higher-order features (e.g., brightness, contrast, ...) and, hence, we say they evaluate the ***perceptual similarity*** of images. Some commonly used metrics are:\n",
    "- ***Pixel-wise similarity***: `Peak Signal-to-Noise Ratio (PSNR)`, `Pearson's Correlation Coefficient`.\n",
    "- ***Perceptual similarity***: `Structural similarity index measure (SSIM)` with its multi-scale variant `(MS-SSIM)`, and our variant for microscopy `MicroSSIM` (paper: [link](https://arxiv.org/abs/2408.08747)) with its multi-scale variant `(MicroMS3IM)`, `Learned Perceptual Image Patch Similarity (LPIPS)`, `Fr√©chet Inception Distance (FID)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\"><h4><b>Task 2.3: Compute metrics</b></h4>\n",
    "\n",
    "Here, your task is to select appropriate metrics to use for the quantitative evaluation among the available ones.\n",
    "\n",
    "*Hint*: there are no absolutely good and bad metrics. All the metrics are useful! They key is to understand *what they are telling you*.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34eb6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comment out the metrics you don't want to use\n",
    "METRICS = [\n",
    "    \"PSNR\",\n",
    "    \"Pearson\",\n",
    "    \"SSIM\",\n",
    "    \"MS-SSIM\",\n",
    "    \"MicroSSIM\",\n",
    "    \"MicroMS3IM\",\n",
    "    \"LPIPS\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**NOTE**: as ground truth reference for computing the metrics, we will use the high-SNR images obtained with long exposure (500ms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, _, gt_test_dset, _ = create_train_val_datasets(\n",
    "    datapath=ROOT_DIR / \"data/500ms\",\n",
    "    train_config=train_data_config,\n",
    "    val_config=val_data_config,\n",
    "    test_config=val_data_config,\n",
    "    load_data_func=load_data_func,\n",
    ")\n",
    "gt_target = gt_test_dset._data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Metrics computation (NOTE: should you get any warnings, don't worry, computation is still ok!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics_dict = compute_metrics(gt_target, stitched_predictions, metrics=METRICS)\n",
    "show_metrics(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\"><h4><b>Question 2.3.</b></h4>\n",
    "\n",
    "- Do you spot inconsistencies between your qualitative judgement and the computed metrics? Did you expect something different?\n",
    "- Which metrics are the most informative/interpretable?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\"><h2><b>Checkpoint 4: Qualitative and Quantitative evaluation</b></h2>\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BONUS: visualize difference between samples\n",
    "\n",
    "Here we compute a pair of posterior samples, to see how they are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgsz = 3\n",
    "examplecount = 3\n",
    "ncols = 6\n",
    "nrows = stitched_predictions.shape[-1] * examplecount\n",
    "_, ax = plt.subplots(\n",
    "    figsize=(imgsz * ncols, imgsz * nrows),\n",
    "    ncols=ncols,\n",
    "    nrows=nrows,\n",
    "    constrained_layout=True,\n",
    ")\n",
    "\n",
    "show_sampling(test_dset, model, ax=ax[:3])\n",
    "show_sampling(test_dset, model, ax=ax[3:6])\n",
    "show_sampling(test_dset, model, ax=ax[6:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcac2b65",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\"><h4><b>Bonus Question</b></h4>\n",
    "\n",
    "How do you think we could measure the model's confidence using these samples?\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
