{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "# Train a Noise2Void network\n",
    "\n",
    "Both the CARE network and Noise2Noise network you trained in part 1 and 2 require that you acquire additional data for the purpose of denoising. For CARE we used a paired acquisition with high SNR, for Noise2Noise we had paired noisy acquisitions.\n",
    "We will now train a Noise2Void network from single noisy images.\n",
    "\n",
    "This notebook uses a single image from the SEM data from the Noise2Noise notebook, but as you'll see in Task 3.1 if you brought your own raw data you should adapt the notebook to use that instead.\n",
    "\n",
    "We now use the Noise2Void ([n2v](https://github.com/juglab/n2v)) library instead of csbdeep/care, but don't worry - they're pretty similar. Make sure you have the right kernel (`n2v`) selected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><h4>\n",
    "    TASK 3.1</h4>\n",
    "    <p>\n",
    "This notebook uses a single image from the SEM data from the Noise2Noise notebook.\n",
    "\n",
    "If you brought your own raw data, use that instead!\n",
    "The only requirement is that the noise in your data is pixel-independent and zero-mean. If you're unsure whether your data fulfills that requirement or you don't yet understand why it is necessary ask one of us to discuss!\n",
    "\n",
    "If you don't have suitable data of your own, feel free to find some online or ask your fellow course participants. You can however also stick with the SEM data provided here and compare the results to what you achieved with Noise2Noise in the previous part.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import all our dependencies.\n",
    "from n2v.models import N2VConfig, N2V\n",
    "import numpy as np\n",
    "from csbdeep.utils import plot_history\n",
    "from n2v.utils.n2v_utils import manipulate_val_data\n",
    "from n2v.internals.N2V_DataGenerator import N2V_DataGenerator\n",
    "from matplotlib import pyplot as plt\n",
    "import urllib\n",
    "import os\n",
    "from skimage.metrics import structural_similarity, peak_signal_noise_ratio\n",
    "from tifffile import imread\n",
    "import zipfile\n",
    "%load_ext tensorboard\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Part 1: Prepare data\n",
    "We downloaded the data during the Noise2Noise exercise, but let's make sure it's there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you get an error go to the Noise2Noise notebook and download the data\n",
    "assert os.path.exists(\"data/SEM/train/train.tif\")\n",
    "assert os.path.exists(\"data/SEM/test/test.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a N2V_DataGenerator object to help load data and extract patches for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = N2V_DataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data generator provides two methods for loading data: `load_imgs_from_directory` and `load_imgs`. Let's look at the docstring of `load_imgs` to figure out how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?N2V_DataGenerator.load_imgs_from_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pass in the directory containing our image files (`\"data/SEM/train\"`), our image matches the default filter (`\"*.tif\"`) so we do not need to specify that. But our tif image is a stack of several images, so as dims we need to specify `\"TYX\"`.\n",
    "If you're using your own data adapt this part to match your use case. You can also have a look at `load_imgs`. Or load your images manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imgs = datagen.load_imgs_from_directory(\"data/SEM/train\", dims=\"TYX\")\n",
    "print(f\"Loaded {len(imgs)} images.\")\n",
    "print(f\"First image has shape {imgs[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method returned a list of images, as per the doc string the dimensions of each are \"SYXC\". However, we only want to use one of the images here since Noise2Void is designed to work with just one acquisition of the sample. Let's use the first image at $1\\mu s$ scantime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imgs = [img[2:3,:,:,:] for img in imgs]\n",
    "print(f\"First image has shape {imgs[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For generating patches the datagenerator provides the methods `generate_patches` and `generate_patches_from_list`. As before, let's have a quick look at the docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?N2V_DataGenerator.generate_patches_from_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = datagen.generate_patches_from_list(imgs, shape=(96,96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and validation\n",
    "n_train = int(round(.9 * patches.shape[0]))\n",
    "X, X_val = patches[:n_train,...], patches[n_train:,...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per usual, let's look at a training and validation patch to make sure everything looks okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X[np.random.randint(X.shape[0]),...,0], cmap=\"gray_r\")\n",
    "plt.title(\"Training patch\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(X_val[np.random.randint(X_val.shape[0]),...,0], cmap=\"gray_r\")\n",
    "plt.title(\"Validation patch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Part 2: Configure and train the Noise2Void Network\n",
    "\n",
    "Noise2Void comes with a special config-object, where we store network-architecture and training specific parameters. See the docstring of the <code>N2VConfig</code> constructor for a description of all parameters.\n",
    "\n",
    "When creating the config-object, we provide the training data <code>X</code>. From <code>X</code> we extract <code>mean</code> and <code>std</code> that will be used to normalize all data before it is processed by the network. We also extract the dimensionality and number of channels from <code>X</code>.\n",
    "\n",
    "Compared to supervised training (i.e. traditional CARE), we recommend to use N2V with an increased <code>train_batch_size</code> and <code>batch_norm</code>.\n",
    "To keep the network from learning the identity we have to manipulate the input pixels during training. For this we have the parameter <code>n2v_manipulator</code> with default value <code>'uniform_withCP'</code>. Most pixel manipulators will compute the replacement value based on a neighborhood. With <code>n2v_neighborhood_radius</code> we can control its size. \n",
    "\n",
    "Other pixel manipulators:\n",
    "* normal_withoutCP: samples the neighborhood according to a normal gaussian distribution, but without the center pixel\n",
    "* normal_additive: adds a random number to the original pixel value. The random number is sampled from a gaussian distribution with zero-mean and sigma = <code>n2v_neighborhood_radius</code>\n",
    "* normal_fitted: uses a random value from a gaussian normal distribution with mean equal to the mean of the neighborhood and standard deviation equal to the standard deviation of the neighborhood.\n",
    "* identity: performs no pixel manipulation\n",
    "\n",
    "For faster training multiple pixels per input patch can be manipulated. In our experiments we manipulated about 0.198% of the input pixels per patch. For a patch size of 64 by 64 pixels this corresponds to about 8 pixels. This fraction can be tuned via <code>n2v_perc_pix</code>.\n",
    "\n",
    "For Noise2Void training it is possible to pass arbitrarily large patches to the training method. From these patches random subpatches of size <code>n2v_patch_shape</code> are extracted during training. Default patch shape is set to (64, 64).  \n",
    "\n",
    "In the past we experienced bleedthrough artifacts between channels if training was terminated to early. To counter bleedthrough we added the `single_net_per_channel` option, which is turned on by default. In the back a single U-Net for each channel is created and trained independently, thereby removing the possiblity of bleedthrough. <br/>\n",
    "Essentially the network gets multiplied by the number of channels, which increases the memory requirements. If your GPU gets too small, you can always split the channels manually and train a network for each channel one after another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><h4>\n",
    "    TASK 3.2</h4>\n",
    "    <p>\n",
    "As suggested look at the docstring of the N2VConfig and then generate a configuration for your Noise2Void network, and choose a name to identify your model by.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?N2VConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###TODO###\n",
    "config = N2VConfig(\n",
    ")\n",
    "vars(config)\n",
    "model_name = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model\n",
    "model = N2V(config, model_name, basedir=\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model and monitor the progress in tensorboard.\n",
    "Adapt the command below as you did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.train(X, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(history.history.keys())))\n",
    "plt.figure(figsize=(16,5))\n",
    "plot_history(history, [\"loss\", \"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Part 3: Prediction\n",
    "\n",
    "Similar to CARE a previously trained model is loaded by creating a new N2V-object without providing a `config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = N2V(config=None, name=model_name, basedir=\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a $1\\mu s$ scantime test images and denoise them using our network and like before we'll use the high SNR image to make a quantitative comparison. If you're using your own data and don't have an equivalent you can ignore that part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = imread(\"data/SEM/test/test.tif\")[2,...]\n",
    "test_img_highSNR = imread(\"data/SEM/test/test.tif\")[-1,...]\n",
    "print(f\"Loaded test image with shape {test_img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_denoised = model.predict(test_img, axes=\"YX\", n_tiles=(2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,30))\n",
    "plt.subplot(2,3,1)\n",
    "plt.imshow(test_img, cmap=\"gray_r\")\n",
    "plt.title(\"Noisy test image\")\n",
    "plt.subplot(2,3,4)\n",
    "plt.imshow(test_img[2000:2200,500:700], cmap=\"gray_r\")\n",
    "plt.subplot(2,3,2)\n",
    "plt.imshow(test_denoised, cmap=\"gray_r\")\n",
    "plt.title(\"Denoised test image\")\n",
    "plt.subplot(2,3,5)\n",
    "plt.imshow(test_denoised[2000:2200,500:700], cmap=\"gray_r\")\n",
    "plt.subplot(2,3,3)\n",
    "plt.imshow(test_img_highSNR, cmap=\"gray_r\")\n",
    "plt.title(\"High SNR image (4x5us)\")\n",
    "plt.subplot(2,3,6)\n",
    "plt.imshow(test_img_highSNR[2000:2200,500:700], cmap=\"gray_r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\"><h4>\n",
    "    TASK 3.3</h4>\n",
    "    <p>\n",
    "\n",
    "If you're using the SEM data (or happen to have a high SNR version of the image you predicted from) compare the structural similarity index and peak signal to noise ratio (wrt the high SNR image) of the noisy input image and the predicted image.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###TODO###\n",
    "ssi_input = #TODO\n",
    "ssi_restored = #TODO\n",
    "print(f\"Structural similarity index (higher is better) wrt average of 4x5us images: \\n\"\n",
    "      f\"Input: {ssi_input} \\n\"\n",
    "      f\"Prediction: {ssi_restored}\")\n",
    "psnr_input = #TODO\n",
    "psnr_restored = #TODO\n",
    "print(f\"Peak signal-to-noise ratio (higher is better) wrt average of 4x5us images:\\n\"\n",
    "      f\"Input: {psnr_input} \\n\"\n",
    "      f\"Prediction: {psnr_restored}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<hr style=\"height:2px;\">\n",
    "<div class=\"alert alert-block alert-success\"><h1>\n",
    "    Congratulations!</h1>\n",
    "    <p>\n",
    "    <b>You have reached the third checkpoint of this exercise! Please mark your progress on slack!</b>\n",
    "    </p>\n",
    "    <p>\n",
    "    If there's still time, check out the bonus exercise.\n",
    "    </p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:n2v]",
   "language": "python",
   "name": "conda-env-n2v-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
