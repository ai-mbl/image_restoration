{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "# Train your first CARE network (supervised)\n",
    "\n",
    "In this first example we will train a CARE network for a 2D denoising and upsampling task, where corresponding pairs of low and high signal-to-noise ratio (SNR) images of cells are available. Here the high SNR images are acquistions of Human U2OS cells taken from the [Broad Bioimage Benchmark Collection](https://data.broadinstitute.org/bbbc/BBBC006/) and the low SNR images were created by synthetically adding *strong read-out and shot-noise* and applying *pixel binning* of 2x2, thus mimicking acquisitions at a very low light level.   \n",
    "\n",
    "![](nb_material/denoising_binning_overview.png)\n",
    "\n",
    "\n",
    "Each image pair should be registered, which in a real application setting is best achieved by acquiring both stacks _interleaved_, i.e. as different channels that correspond to the different exposure/laser settings. \n",
    "\n",
    "Since the image pairs were synthetically created in this example, they are already aligned perfectly.\n",
    "\n",
    "To train a denoising network we will use the [CSB Deep Repo](https://github.com/CSBDeep/CSBDeep). This notebook has a very similar structure to the examples you can find there.\n",
    "More documentation is available at http://csbdeep.bioimagecomputing.com/doc/.\n",
    "\n",
    "There will be no tasks to fill in in this exercise, but go through each cell and try to understand what's going on - it will help you in the next part! We also put some questions along the way. For some of them you might need to dig a bit deeper.\n",
    "\n",
    "You'll want to select the kernel for the `'care'` environment for this exercise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div> <img src=\"nb_material/change_kernel.png\" alt=\"How to change kernel with nb_conda\" style=\"width: 1600px;\"/></div> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals, absolute_import, division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from tifffile import imread\n",
    "from csbdeep.utils import axes_dict, download_and_extract_zip_file, normalize, Path, plot_history, plot_some\n",
    "from csbdeep.utils.tf import limit_gpu_memory\n",
    "from csbdeep.data import RawData, create_patches, no_background_patches, norm_percentiles, sample_percentiles\n",
    "from csbdeep.io import load_training_data, save_tiff_imagej_compatible\n",
    "from csbdeep.models import Config, CARE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Part 1: Training Data Generation\n",
    "Network training usually happens on batches of smaller sized images than the ones recorded on a microscopy. In this first part of the exercise, we will load all of the image data and chop it into smaller pieces, a.k.a. patches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download example data\n",
    "\n",
    "First we download some example data, consisting of low-SNR and high-SNR 3D images of Tribolium.  \n",
    "Note that `GT` stands for ground truth and represents high signal-to-noise ratio (SNR) stacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "download_and_extract_zip_file (\n",
    "    url       =  'https://zenodo.org/record/6973411/files/care_denoising_upsampling.zip?download=1',\n",
    "    targetdir = 'data/U2OS/',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the data set is already split into a **train** and **test** set, each containing (synthetically generated) low SNR (\"low\") and corresponding high SNR (\"GT\") images.\n",
    "\n",
    "Let's look at an example pair of training images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = imread('data/train/GT/img_0010.tif')\n",
    "x = imread('data/train/low/img_0010.tif')\n",
    "print('GT image size =', x.shape)\n",
    "print('low image size =', y.shape)\n",
    "\n",
    "plt.figure(figsize=(13,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(x, cmap  =\"magma\")\n",
    "plt.colorbar()\n",
    "plt.title(\"low\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(y, cmap  =\"magma\")\n",
    "plt.colorbar()\n",
    "plt.title(\"high\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data for CARE\n",
    "\n",
    "We first need to create a `RawData` object, which defines how to get the pairs of low/high SNR stacks and the semantics of each axis (e.g. which one is considered a color channel, etc.).\n",
    "\n",
    "Here we have two folders \"low\" and \"GT\", where corresponding low and high-SNR stacks are TIFF images with identical filenames.\n",
    "\n",
    "For this case, we can simply use `RawData.from_folder` and set `axes = 'YX'` to indicate the semantic order of the image axes, i.e. we have two-dimensional images in standard xy layout.\n",
    "\n",
    "In general the names for the axes are:\n",
    "\n",
    "X: columns, Y: rows, Z: planes, C: channels, T: frames/time, (S: samples/images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = RawData.from_folder (\n",
    "    basepath    = 'data/U2OS/train',\n",
    "    source_dirs = ['low'],\n",
    "    target_dir  = 'GT',\n",
    "    axes        = 'YX',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From corresponding images, we now generate some 2D patches to use for training. \n",
    "\n",
    "As a general rule, use a *patch size* that is a power of two along all axes, or at least divisible by 8.  Typically, you should use more patches the more trainings images you have. \n",
    "\n",
    "An important aspect is *data normalization*, i.e. the rescaling of corresponding patches to a dynamic range of ~ (0,1). By default, this is automatically provided via percentile normalization, which can be adapted if needed.\n",
    "\n",
    "By default, patches are sampled from *non-background regions* (i.e. that are above a relative threshold). We will disable this for the current example as most image regions already contain foreground pixels and thus set the threshold to 0. See the documentation of `create_patches` for details. \n",
    "\n",
    "Note that returned values `(X, Y, XY_axes)` by `create_patches` are not to be confused with the image axes X and Y.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, XY_axes = create_patches (\n",
    "    raw_data            = raw_data,\n",
    "    patch_size          = (128,128),\n",
    "    patch_filter        = no_background_patches(0),\n",
    "    n_patches_per_image = 2,\n",
    "    save_file           = 'data/U2OS/my_training_data.npz',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape == Y.shape\n",
    "print(\"shape of X,Y =\", X.shape)\n",
    "print(\"axes  of X,Y =\", XY_axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show\n",
    "\n",
    "This shows some of the generated patch pairs (odd rows: *source*, even rows: *target*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    plt.figure(figsize=(16,4))\n",
    "    sl = slice(8*i, 8*(i+1)), 0\n",
    "    plot_some(X[sl],Y[sl],title_list=[np.arange(sl[0].start,sl[0].stop)]) #convenience function provided by CSB Deeo\n",
    "    plt.show()\n",
    "None;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "1. Where is the training data located? \n",
    "2.How does data have to be stored so that CSBDeep will find and load it correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Part 2: Training the network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training data\n",
    "\n",
    "Load the patches generated in part 1, use 10% as validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X,Y), (X_val,Y_val), axes = load_training_data('data/U2OS/my_training_data.npz', validation_split=0.1, verbose=True)\n",
    "\n",
    "c = axes_dict(axes)['C']\n",
    "n_channel_in, n_channel_out = X.shape[c], Y.shape[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plot_some(X_val[:5],Y_val[:5])\n",
    "plt.suptitle('5 example validation patches (top row: source, bottom row: target)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the CARE model\n",
    "Before we construct the actual CARE model, we have to define its configuration via a `Config` object, which includes \n",
    "* parameters of the underlying neural network,\n",
    "* the learning rate,\n",
    "* the number of parameter updates per epoch,\n",
    "* the loss function, and\n",
    "* whether the model is probabilistic or not.\n",
    "\n",
    "![](nb_material/carenet.png)\n",
    "\n",
    "The defaults should be sensible in many cases, so a change should only be necessary if the training process fails.  \n",
    "\n",
    "<span style=\"color:red;font-weight:bold;\">Important</span>: Note that for this notebook we use a very small number of update steps per epoch for immediate feedback, whereas this number should be increased considerably (e.g. `train_steps_per_epoch=400`) to obtain a well-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(axes, n_channel_in, n_channel_out, train_batch_size=8, train_steps_per_epoch=200, train_epochs=20)\n",
    "vars(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a CARE model with the chosen configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = CARE(config, 'my_CARE_model', basedir='models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a summary of all the layers in the model and the number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.keras_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model will likely take some time. We recommend to monitor the progress with [TensorBoard](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard), which allows you to inspect the losses during training.\n",
    "Furthermore, you can look at the predictions for some of the validation images, which can be helpful to recognize problems early on.\n",
    "\n",
    "We start tensorboard in the notebook (you can also launch it outside the notebook if you prefer by changing the `%` to `!`) to then monitor the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir models --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.train(X,Y, validation_data=(X_val,Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot final training history (available in TensorBoard during training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(history.history.keys())))\n",
    "plt.figure(figsize=(16,5))\n",
    "plot_history(history,['loss','val_loss'],['mse','val_mse','mae','val_mae']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Example results for validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "_P = model.keras_model.predict(X_val[:5])\n",
    "if config.probabilistic:\n",
    "    _P = _P[...,:(_P.shape[-1]//2)]\n",
    "plot_some(X_val[:5],Y_val[:5],_P,pmax=99.5)\n",
    "plt.suptitle('5 example validation patches\\n'      \n",
    "             'top row: input (source),  '          \n",
    "             'middle row: target (ground truth),  '\n",
    "             'bottom row: predicted from source');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "1. Where are trained models stored? What models are being stored, how do they differ?\n",
    "2. How does the name of the saved models get specified?\n",
    "3. How can you influence the number of training steps per epoch? What did you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Part 3: Prediction\n",
    "\n",
    "Plot the test stack pair and define its image axes, which will be needed later for CARE prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_test = imread('data/test/GT/img_0010.tif')\n",
    "x_test = imread('data/test/low/img_0010.tif')\n",
    "\n",
    "axes = 'YX'\n",
    "print('image size =', x_test.shape)\n",
    "print('image axes =', axes)\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "plot_some(np.stack([x_test,y_test]),\n",
    "          title_list=[['low','high']]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CARE model\n",
    "\n",
    "Load trained model (located in base directory `models` with name `my_CARE_model`) from disk.  \n",
    "The configuration was saved during training and is automatically loaded when `CARE` is initialized with `config=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CARE(config=None, name='my_CARE_model', basedir='models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply CARE network to raw image\n",
    "Predict the restored image (image will be successively split into smaller tiles if there are memory issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "restored = model.predict(x_test, axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save restored image\n",
    "\n",
    "Save the restored image stack as a ImageJ-compatible TIFF image, i.e. the image can be opened in ImageJ/Fiji with correct axes semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Path('results').mkdir(exist_ok=True)\n",
    "save_tiff_imagej_compatible('results/%s_img_0010.tif' % model.name, restored, axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results\n",
    "Plot the test stack pair and the predicted restored stack (middle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plot_some(np.stack([x_test,restored,y_test]),\n",
    "          title_list=[['low','CARE','GT']], \n",
    "          pmin=2,pmax=99.8);\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "for _x,_name in zip((x_test,restored,y_test),('low','CARE','GT')):\n",
    "    plt.plot(normalize(_x,1,99.7)[180], label = _name, lw = 2)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "### Bonus Questions:\n",
    "Feel free to skip these\n",
    "1. How would you load an existing CARE network and continue to train it? (This is _not_ done in this notebook)\n",
    "2. How would you go about saving the new model separately, under a new name?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<hr style=\"height:2px;\">\n",
    "\n",
    "# Congratulations!\n",
    "__You have reached the first checkpoint of this exercise! Please mark your progress on slack and move on to the next part!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:care] *",
   "language": "python",
   "name": "conda-env-care-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
