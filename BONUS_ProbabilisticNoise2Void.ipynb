{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "# Train Probabilistic Noise2Void\n",
    "\n",
    "Probabilistic Noise2Void, just as N2V, allows training from single noisy images.\n",
    "\n",
    "In order to get some additional quality squeezed out of your noisy input data, PN2V employs an additional noise model which can either be measured directly at your microscope or approximated by a process called ‘bootstrapping’.\n",
    "Below we will give you a noise model for the first network to train and then bootstrap one, so you can apply PN2V to your own data (in this course we simply don’t have the means to also do the microscopy bits to record a suitable noise model for your data…)\n",
    "\n",
    "Note: The PN2V implementation is written in pytorch, not Keras/TF.  \n",
    "Note: PN2V experienced multiple “updates” regarding noise model representations. Hence, the original PN2V repository is not any more the one we suggest to use… (despite it of course working just as described in the original publication…)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Generate a Noise Model using Calibration Data \n",
    "\n",
    "We will use pairs of noisy calibration observations $x_i$ and clean signal $s_i$ (created by averaging these noisy, calibration images) to estimate the conditional distribution $p(x_i|s_i)$. Histogram-based and Gaussian Mixture Model-based noise models are generated and saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\") \n",
    "from torch.distributions import normal\n",
    "import matplotlib.pyplot as plt, numpy as np, pickle\n",
    "from scipy.stats import norm\n",
    "from tifffile import imread\n",
    "import sys\n",
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "sys.path.append('../PPN2V')\n",
    "import unet.model\n",
    "from unet.model import UNet\n",
    "from pn2v import *\n",
    "import pn2v.gaussianMixtureNoiseModel\n",
    "import pn2v.histNoiseModel\n",
    "import pn2v.prediction\n",
    "import pn2v.training\n",
    "from pn2v.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "Download the data from https://zenodo.org/record/5156913/files/Convallaria_diaphragm.zip?download=1. Here we show the pipeline for Convallaria dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "\n",
    "\n",
    "if not os.path.isdir('data'):\n",
    "    os.mkdir('data')\n",
    "\n",
    "zipPath=\"data/Convallaria_diaphragm.zip\"\n",
    "if not os.path.exists(zipPath):  \n",
    "    data = urllib.request.urlretrieve('https://zenodo.org/record/5156913/files/Convallaria_diaphragm.zip?download=1', zipPath)\n",
    "    with zipfile.ZipFile(zipPath, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The noise model is a characteristic of your camera. The downloaded data folder contains a set of calibration images (For the Convallaria dataset, it is ```20190726_tl_50um_500msec_wf_130EM_FD.tif``` and the data to be denoised is named ```20190520_tl_25um_50msec_05pc_488_130EM_Conv.tif```). We can either bin the noisy - GT pairs (obtained from noisy calibration images) as a 2-D histogram or fit a GMM distribution to obtain a smooth, parametric description of the noise model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify ```path```, ```dataName```,  ```n_gaussian```, ```n_coeff```\n",
    "Ensure that ```dataName``` is set same as in ```1_N2VTraining.ipynb```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"data/Convallaria_diaphragm/\"\n",
    "dataName = \"convallaria\" # Name of the noise model \n",
    "calibration_fn = \"20190726_tl_50um_500msec_wf_130EM_FD.tif\"\n",
    "noisy_fn = \"20190520_tl_25um_50msec_05pc_488_130EM_Conv.tif\"\n",
    "n_gaussian = 3 # Number of gaussians to use for Gaussian Mixture Model\n",
    "n_coeff = 2 # No. of polynomial coefficients for parameterizing the mean, standard deviation and weight of Gaussian components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation= imread(path+noisy_fn) # Load the appropriate data\n",
    "\n",
    "nameHistNoiseModel ='HistNoiseModel_'+dataName+'_'+'calibration'\n",
    "nameGMMNoiseModel = 'GMMNoiseModel_'+dataName+'_'+str(n_gaussian)+'_'+str(n_coeff)+'_'+'calibration'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data contains 100 images of a static sample.\n",
    "# We estimate the clean signal by averaging all images.\n",
    "\n",
    "signal=np.mean(observation[:, ...],axis=0)[np.newaxis,...]\n",
    "\n",
    "# Let's look the raw data and our pseudo ground truth signal\n",
    "print(signal.shape)\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(label='average (ground truth)')\n",
    "plt.imshow(signal[0],cmap='gray')\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(label='single raw image')\n",
    "plt.imshow(observation[0],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways of generating a noise model for PN2V: creating a histogram of the noisy pixels based on the averaged GT or using a gaussian mixture model (GMM). You can pick which one you wanna use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;\">\n",
    "\n",
    "### Creating the Histogram Noise Model\n",
    "Using the raw pixels $x_i$, and our averaged GT $s_i$, we are now learning a histogram based noise model. It describes the distribution $p(x_i|s_i)$ for each $s_i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the range of values we want to cover with our model.\n",
    "# The pixel intensities in the images you want to denoise have to lie within this range.\n",
    "# The dataset is clipped to values between 0 and 255.\n",
    "minVal, maxVal = 234, 7402\n",
    "bins = 256\n",
    "\n",
    "# We are creating the histogram.\n",
    "# This can take a minute.\n",
    "histogram = pn2v.histNoiseModel.createHistogram(bins, minVal, maxVal, observation,signal)\n",
    "\n",
    "# Saving histogram to disc.\n",
    "np.save(path+nameHistNoiseModel+'.npy', histogram)\n",
    "histogramFD=histogram[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the histogram-based noise model.\n",
    "plt.xlabel('Observation Bin')\n",
    "plt.ylabel('Signal Bin')\n",
    "plt.imshow(histogramFD**0.25, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;\">\n",
    "\n",
    "### Creating the GMM noise model\n",
    "Using the raw pixels $x_i$, and our averaged GT $s_i$, we are now learning a GMM based noise model. It describes the distribution $p(x_i|s_i)$ for each $s_i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_signal=np.min(signal)\n",
    "max_signal=np.max(signal)\n",
    "print(\"Minimum Signal Intensity is\", min_signal)\n",
    "print(\"Maximum Signal Intensity is\", max_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating the noise model training for `n_epoch=2000` and `batchSize=250000` works the best for `Convallaria` dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussianMixtureNoiseModel = pn2v.gaussianMixtureNoiseModel.GaussianMixtureNoiseModel(min_signal = min_signal, max_signal =max_signal, path=path, weight = None, n_gaussian = n_gaussian, n_coeff = n_coeff, min_sigma = 50, device = device)\n",
    "gaussianMixtureNoiseModel.train(signal, observation, batchSize = 250000, n_epochs = 2000, learning_rate=0.1, name = nameGMMNoiseModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;\">\n",
    "\n",
    "### Visualizing the Histogram-based and GMM-based noise models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotProbabilityDistribution(signalBinIndex=170, histogram=histogramFD, gaussianMixtureNoiseModel=gaussianMixtureNoiseModel, min_signal=minVal, max_signal=maxVal, n_bin= bins, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Generate a Noise Model by Bootstrapping\n",
    "We will use pairs of noisy images $x_i$ and clean pseudo ground truth $s_i$ generated with Noise2Void. So first, we need to train a N2V model (now with pytorch).\n",
    "\n",
    "\n",
    "(created by performing Noise2Void in notebooks `1_N2VTraining.ipynb`) to estimate the conditional distribution $p(x_i|s_i)$. Histogram-based and Gaussian Mixture Model-based noise models are generated and saved. No additional, calibration data is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = imread(path+noisy_fn)\n",
    "nameModel=dataName+'_n2v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2v_net = UNet(1, depth=3)\n",
    "train_data=data[:-5].copy()\n",
    "val_data=data[-5:].copy()\n",
    "np.random.shuffle(train_data)\n",
    "np.random.shuffle(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainHist, valHist = pn2v.training.trainNetwork(net=n2v_net, trainData=train_data, valData=val_data,\n",
    "                                           postfix= nameModel, directory=path, noiseModel=None,\n",
    "                                           device=device, numOfEpochs= 200, stepsPerEpoch = 10, \n",
    "                                           virtualBatchSize=20, batchSize=1, learningRate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the training and validation loss\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(valHist, label='validation loss')\n",
    "plt.plot(trainHist, label='training loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now run the N2V model to create pseudo groundtruth.\n",
    "results=[]\n",
    "meanRes=[]\n",
    "resultImgs=[]\n",
    "inputImgs=[]\n",
    "dataTest = observation\n",
    "\n",
    "for index in range(dataTest.shape[0]):\n",
    "\n",
    "    im=dataTest[index]\n",
    "    # We are using tiling to fit the image into memory\n",
    "    # If you get an error try a smaller patch size (ps)\n",
    "    means = pn2v.prediction.tiledPredict(im, n2v_net, ps=256, overlap=48,\n",
    "                                            device=device, noiseModel=None)\n",
    "    resultImgs.append(means)\n",
    "    inputImgs.append(im)\n",
    "    if index%10 == 0:\n",
    "        print (\"image:\", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In bootstrap mode, we estimate pseudo GT by using N2V denoised images.\n",
    "signal = np.array(resultImgs)   \n",
    "# Let's look the raw data and our pseudo ground truth signal\n",
    "print(signal.shape)\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(label='pseudo GT (generated by N2V denoising)')\n",
    "plt.imshow(signal[0],cmap='gray')\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(label='single raw image')\n",
    "plt.imshow(observation[0],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have pseudoGT, you can pick again between a histogram based noise model and a GMM noise model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;\">\n",
    "\n",
    "### Creating the Histogram Noise Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the range of values we want to cover with our model.\n",
    "# The pixel intensities in the images you want to denoise have to lie within this range.\n",
    "# The dataset is clipped to values between 0 and 255.\n",
    "minVal, maxVal = 234, 7402\n",
    "bins = 256\n",
    "\n",
    "# We are creating the histogram.\n",
    "# This can take a minute.\n",
    "histogram = pn2v.histNoiseModel.createHistogram(bins, minVal, maxVal, observation, signal)\n",
    "\n",
    "# Saving histogram to disc.\n",
    "np.save(path+nameHistNoiseModel+'.npy', histogram)\n",
    "histogramFD=histogram[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the histogram-based noise model\n",
    "plt.xlabel('Observation Bin')\n",
    "plt.ylabel('Signal Bin')\n",
    "plt.imshow(histogramFD**0.25, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;\">\n",
    "\n",
    "### Creating the GMM noise model\n",
    "Using the raw pixels $x_i$, and our averaged GT $s_i$, we are now learning a GMM based noise model. It describes the distribution $p(x_i|s_i)$ for each $s_i$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_signal=np.percentile(signal, 0.5)\n",
    "max_signal=np.percentile(signal, 99.5)\n",
    "print(\"Minimum Signal Intensity is\", min_signal)\n",
    "print(\"Maximum Signal Intensity is\", max_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating the noise model training for `n_epoch=2000` and `batchSize=250000` works the best for `Convallaria` dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussianMixtureNoiseModel = pn2v.gaussianMixtureNoiseModel.GaussianMixtureNoiseModel(min_signal = min_signal, max_signal = max_signal, path=path, weight = None, n_gaussian = n_gaussian, n_coeff = n_coeff, device = device, min_sigma = 50)\n",
    "gaussianMixtureNoiseModel.train(signal, observation, batchSize = 250000, n_epochs = 2000, learning_rate = 0.1, name = nameGMMNoiseModel, lowerClip = 0.5, upperClip = 99.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Histogram-based and GMM-based noise models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotProbabilityDistribution(signalBinIndex=170, histogram=histogramFD, gaussianMixtureNoiseModel=gaussianMixtureNoiseModel, min_signal=minVal, max_signal=maxVal, n_bin= bins, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## PN2V Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the noise model of your choice to train PN2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameNoiseModel ='HistNoiseModel_'+dataName+'_'+'calibration'\n",
    "#nameNoiseModel='GMMNoiseModel_'+dataName+'_'+str(3)+'_'+str(2)+'_'+'calibration'\n",
    "# nameNoiseModel ='HistNoiseModel_'+dataName+'_'+'bootstrap'\n",
    "#nameNoiseModel='GMMNoiseModel_'+dataName+'_'+str(3)+'_'+str(2)+'_'+'bootstrap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namePN2VModel=nameNoiseModel\n",
    "if('HistNoiseModel' in namePN2VModel):\n",
    "    histogram = np.load(path+nameNoiseModel+'.npy') \n",
    "    noiseModel= pn2v.histNoiseModel.NoiseModel(histogram, device=device)\n",
    "elif('GMMNoiseModel' in namePN2VModel):\n",
    "    params= np.load(path+nameNoiseModel+'.npz')\n",
    "    noiseModel = pn2v.gaussianMixtureNoiseModel.GaussianMixtureNoiseModel(params = params, device = device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a network with 800 output channels that are interpreted as samples from the prior.\n",
    "pn2v_net = UNet(800, depth=3)\n",
    "\n",
    "# Start training.\n",
    "trainHist, valHist = pn2v.training.trainNetwork(net=pn2v_net, trainData=train_data, valData=val_data,\n",
    "                                           postfix=namePN2VModel, directory=path, noiseModel=noiseModel,\n",
    "                                           device=device, numOfEpochs= 200, stepsPerEpoch=5, \n",
    "                                           virtualBatchSize=20, batchSize=1, learningRate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## PN2V Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTest=observation[:,:512,:512]\n",
    "# We are loading only a sub image to speed up computation\n",
    "\n",
    "# We estimate the ground truth by averaging.\n",
    "dataTestGT=np.mean(dataTest[:,...],axis=0)[np.newaxis,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are processing data and calculating PSNR values.\n",
    "results=[]\n",
    "meanRes=[]\n",
    "resultImgs=[]\n",
    "inputImgs=[]\n",
    "\n",
    "# We iterate over all test images.\n",
    "for index in range(dataTest.shape[0]):\n",
    "    \n",
    "    im=dataTest[index]\n",
    "    gt=dataTestGT[0] # The ground truth is the same for all images\n",
    "    \n",
    "    # We are using tiling to fit the image into memory\n",
    "    # If you get an error try a smaller patch size (ps)\n",
    "    means, mseEst = pn2v.prediction.tiledPredict(im, pn2v_net ,ps=192, overlap=48,\n",
    "                                            device=device, noiseModel=noiseModel)\n",
    "    \n",
    "    resultImgs.append(mseEst)\n",
    "    inputImgs.append(im)\n",
    "\n",
    "    rangePSNR=np.max(gt)-np.min(gt)\n",
    "    psnr=PSNR(gt, mseEst,rangePSNR )\n",
    "    psnrPrior=PSNR(gt, means,rangePSNR )\n",
    "    results.append(psnr)\n",
    "    meanRes.append(psnrPrior)\n",
    "\n",
    "    print (\"image:\",index)\n",
    "    print (\"PSNR input\",PSNR(gt, im, rangePSNR))\n",
    "    print (\"PSNR prior\",psnrPrior) # Without info from masked pixel\n",
    "    print (\"PSNR mse\",psnr) # MMSE estimate using the masked pixel\n",
    "    print ('-----------------------------------')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# We display the results for the last test image       \n",
    "vmi=np.percentile(gt,0.01)\n",
    "vma=np.percentile(gt,99)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(label='Input Image')\n",
    "plt.imshow(im, vmax=vma, vmin=vmi, cmap='magma')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(label='Avg. Prior')\n",
    "plt.imshow(means, vmax=vma, vmin=vmi, cmap='magma')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(label='PN2V-MMSE estimate')\n",
    "plt.imshow(mseEst, vmax=vma, vmin=vmi, cmap='magma')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(label='Input Image')\n",
    "plt.imshow(im[100:200,150:250], vmax=vma, vmin=vmi, cmap='magma')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(label='Avg. Prior')\n",
    "plt.imshow(means[100:200,150:250], vmax=vma, vmin=vmi, cmap='magma')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(label='PN2V-MMSE estimate')\n",
    "plt.imshow(mseEst[100:200,150:250], vmax=vma, vmin=vmi, cmap='magma')\n",
    "plt.show()\n",
    "\n",
    "print(\"Avg PSNR Prior:\", np.mean(np.array(meanRes) ), '+-(2SEM)',2*np.std(np.array(meanRes) )/np.sqrt(float(len(meanRes)) ) )\n",
    "print(\"Avg PSNR MMSE:\", np.mean(np.array(results) ),  '+-(2SEM)' ,2*np.std(np.array(results) )/np.sqrt(float(len(results)) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ppn2v]",
   "language": "python",
   "name": "conda-env-ppn2v-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
